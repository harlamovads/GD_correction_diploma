{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/zlovoblachko/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_thingie = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/zlovoblachko/GD_correction_diploma/data/rawfiles\"\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.txt') and filename.split(\".\")[0][-1] == '1':\n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            text = f.read()\n",
    "        with open (os.path.join(directory, filename.split(\".\")[0] + \".ann\"), \"r\") as ann_file:\n",
    "            ann_data = ann_file.readlines()\n",
    "    text_code = filename.split(\".\")[0]\n",
    "    tags = []\n",
    "    for line in ann_data:\n",
    "        if line.startswith(\"T\"):\n",
    "            try:\n",
    "                line = line.split(\"\\t\")\n",
    "                ann_code = line[0]\n",
    "                error_info = line[1].split(\" \")\n",
    "                print\n",
    "                native_tag = error_info[0]\n",
    "                if len(native_tag) == 3:\n",
    "                    raise ValueError\n",
    "                span_start = error_info[1]\n",
    "                span_end = error_info[2]\n",
    "                error_span = line[2][:-1]\n",
    "                for further_line in ann_data:\n",
    "                    if further_line.startswith(\"A\"):\n",
    "                        if ann_code in further_line:\n",
    "                            correction = \"\"\n",
    "                            first_level_tag = \"U\"\n",
    "                            break\n",
    "                    if further_line.startswith(\"#\") and \"lemma\" not in further_line:\n",
    "                        if ann_code in further_line:\n",
    "                            correction = further_line.split(\"\\t\")[2].strip()\n",
    "                            if error_span in correction:\n",
    "                                first_level_tag = \"M\"\n",
    "                            else:\n",
    "                                first_level_tag = \"R\"\n",
    "                            break\n",
    "                tags.append({'error_span': error_span, \n",
    "                             'correction': correction, \n",
    "                             'span_start': span_start, \n",
    "                             'span_end': span_end, \n",
    "                             'native_tag': native_tag, \n",
    "                             'first_level_tag': first_level_tag})\n",
    "            except:\n",
    "                pass\n",
    "    json_thingie.append({\"text\": text, \"tags\": tags})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9383"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_json_thingie = {each['text']: each for each in json_thingie}.values()\n",
    "len(unique_json_thingie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset.jsonl with 9383 entries.\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"dataset.jsonl\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in unique_json_thingie:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Generated {output_filename} with {len(unique_json_thingie)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file = \"dataset.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The chart illustrates the number in percents of overweight children in Canada throughout a 20-years period from 1985 to 2005, while the table demonstrates the percentage of children doing sport exercises regulary over the period from 1990 to 2005.\\nOverall, it can be seen that despite the fact that the number of boys and girls performing exercises has grown considerably by the end of the period, percent of overweight children has increased too.\\nAccording to the graph, boys are more likely to have extra weight in period of 2000-2005, a quater of them had problems with weight in 2005. Girls were going ahead of boys in 1985-1990, then they maintained the same level in 1995, but then the number of outweight boys went up more rapidly.\\nThe table allows to see that interest in physical activity has grown by more than 25% both within boys and girls by 2005.',\n",
       " 'tags': [{'error_span': 'percents',\n",
       "   'correction': 'percent',\n",
       "   'span_start': '36',\n",
       "   'span_end': '44',\n",
       "   'native_tag': 'gram',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': '20-years',\n",
       "   'correction': '20-year',\n",
       "   'span_start': '91',\n",
       "   'span_end': '99',\n",
       "   'native_tag': 'gram',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': 'sport',\n",
       "   'correction': 'sports',\n",
       "   'span_start': '188',\n",
       "   'span_end': '193',\n",
       "   'native_tag': 'Noun_number',\n",
       "   'first_level_tag': 'M'},\n",
       "  {'error_span': 'regulary',\n",
       "   'correction': 'regularly',\n",
       "   'span_start': '204',\n",
       "   'span_end': '212',\n",
       "   'native_tag': 'Spelling',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': 'performing exercises',\n",
       "   'correction': 'doing exercise',\n",
       "   'span_start': '328',\n",
       "   'span_end': '348',\n",
       "   'native_tag': 'gram',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': 'percent',\n",
       "   'correction': 'the percentage',\n",
       "   'span_start': '398',\n",
       "   'span_end': '405',\n",
       "   'native_tag': 'Articles',\n",
       "   'first_level_tag': 'M'},\n",
       "  {'error_span': 'percent',\n",
       "   'correction': 'percentage',\n",
       "   'span_start': '398',\n",
       "   'span_end': '405',\n",
       "   'native_tag': 'Category_confusion',\n",
       "   'first_level_tag': 'M'},\n",
       "  {'error_span': 'period',\n",
       "   'correction': 'the period',\n",
       "   'span_start': '517',\n",
       "   'span_end': '523',\n",
       "   'native_tag': 'Articles',\n",
       "   'first_level_tag': 'M'},\n",
       "  {'error_span': 'of',\n",
       "   'correction': '',\n",
       "   'span_start': '524',\n",
       "   'span_end': '526',\n",
       "   'native_tag': 'disc',\n",
       "   'first_level_tag': 'U'},\n",
       "  {'error_span': ',',\n",
       "   'correction': ';',\n",
       "   'span_start': '536',\n",
       "   'span_end': '537',\n",
       "   'native_tag': 'punct',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': 'quater',\n",
       "   'correction': 'quarter',\n",
       "   'span_start': '540',\n",
       "   'span_end': '546',\n",
       "   'native_tag': 'Spelling',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': 'outweight',\n",
       "   'correction': 'overweight',\n",
       "   'span_start': '702',\n",
       "   'span_end': '711',\n",
       "   'native_tag': 'vocab',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': 'allows',\n",
       "   'correction': 'allows us',\n",
       "   'span_start': '749',\n",
       "   'span_end': '755',\n",
       "   'native_tag': 'gram',\n",
       "   'first_level_tag': 'M'},\n",
       "  {'error_span': 'within',\n",
       "   'correction': 'among',\n",
       "   'span_start': '830',\n",
       "   'span_end': '836',\n",
       "   'native_tag': 'vocab',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': '.',\n",
       "   'correction': 'among',\n",
       "   'span_start': '246',\n",
       "   'span_end': '247',\n",
       "   'native_tag': 'SENT',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': '.',\n",
       "   'correction': 'among',\n",
       "   'span_start': '446',\n",
       "   'span_end': '447',\n",
       "   'native_tag': 'SENT',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': '.',\n",
       "   'correction': 'among',\n",
       "   'span_start': '587',\n",
       "   'span_end': '588',\n",
       "   'native_tag': 'SENT',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': '.',\n",
       "   'correction': 'among',\n",
       "   'span_start': '737',\n",
       "   'span_end': '738',\n",
       "   'native_tag': 'SENT',\n",
       "   'first_level_tag': 'R'},\n",
       "  {'error_span': '.',\n",
       "   'correction': 'among',\n",
       "   'span_start': '859',\n",
       "   'span_end': '860',\n",
       "   'native_tag': 'SENT',\n",
       "   'first_level_tag': 'R'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20-years'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]['text'][int(training_data[0]['tags'][1]['span_start']):int(training_data[0]['tags'][1]['span_end'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import SpanGroup\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy_transformers\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "output_spacy_file = \"dataset.spacy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224532\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for entry in training_data:\n",
    "    if entry['tags']:\n",
    "        for mistake in entry['tags']:\n",
    "             if mistake['first_level_tag']:\n",
    "                 counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for entry in training_data:\n",
    "    try:\n",
    "        text = entry[\"text\"]\n",
    "        annotations = entry[\"tags\"]\n",
    "        doc = nlp(text)\n",
    "        spans = []\n",
    "        for ann in annotations:\n",
    "            start, end = int(ann[\"span_start\"]), int(ann[\"span_end\"])\n",
    "            label = ann[\"first_level_tag\"]\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            spans.append(span)\n",
    "        group = SpanGroup(doc, name=\"sc\", spans=spans)\n",
    "        doc.spans[\"sc\"] = group\n",
    "        doc_bin.add(doc)\n",
    "        counter+=len(annotations)\n",
    "    except:\n",
    "        pass\n",
    "doc_bin.to_disk(output_spacy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221364"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "dev_ratio = 0.2\n",
    "test_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = len(docs)\n",
    "train_end = int(total_docs * train_ratio)\n",
    "dev_end = train_end + int(total_docs * dev_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = docs[:train_end]\n",
    "dev_docs = docs[train_end:dev_end]\n",
    "test_docs = docs[dev_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = {\n",
    "    \"train.spacy\": train_docs,\n",
    "    \"dev.spacy\": dev_docs,\n",
    "    \"test.spacy\": test_docs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, doc_subset in output_files.items():\n",
    "    doc_bin_subset = DocBin()\n",
    "    for doc in doc_subset:\n",
    "        doc_bin_subset.add(doc)\n",
    "    doc_bin_subset.to_disk(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(data: dict):\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    for x, y in data.items():\n",
    "        labels.append(x)\n",
    "        sizes.append(y)\n",
    "    plt.pie(sizes, labels=labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.spacy:\n",
      "- Total Documents: 31742\n",
      "- Total Tokens: 766308\n",
      "- Avg Tokens per Doc: 24.14\n",
      "- Total Spans: 62518\n",
      "- Avg Spans per Doc: 1.97\n",
      "- Tag frequency: [('R', 43418), ('M', 15250), ('U', 3850)]\n",
      "--------------------------------------------------\n",
      "dev.spacy:\n",
      "- Total Documents: 9069\n",
      "- Total Tokens: 218357\n",
      "- Avg Tokens per Doc: 24.08\n",
      "- Total Spans: 17832\n",
      "- Avg Spans per Doc: 1.97\n",
      "- Tag frequency: [('R', 12374), ('M', 4393), ('U', 1065)]\n",
      "--------------------------------------------------\n",
      "test.spacy:\n",
      "- Total Documents: 4536\n",
      "- Total Tokens: 109640\n",
      "- Avg Tokens per Doc: 24.17\n",
      "- Total Spans: 8842\n",
      "- Avg Spans per Doc: 1.95\n",
      "- Tag frequency: [('R', 6068), ('M', 2190), ('U', 584)]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for filename in output_files.keys():\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc_bin = DocBin().from_disk(filename)\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "    total_docs = len(docs)\n",
    "    total_tokens = sum(len(doc) for doc in docs)\n",
    "    total_spans = sum(len(doc.spans.get(\"sc\", [])) for doc in docs)\n",
    "    first_level_counter = Counter()\n",
    "    second_level_counter = Counter()\n",
    "    for doc in docs:\n",
    "        for span in doc.spans.get(\"sc\", []):\n",
    "            label = span.label_\n",
    "            first_level_counter[label] += 1\n",
    "    print(f\"{filename}:\")\n",
    "    print(f\"- Total Documents: {total_docs}\")\n",
    "    print(f\"- Total Tokens: {total_tokens}\")\n",
    "    print(f\"- Avg Tokens per Doc: {total_tokens / total_docs:.2f}\")\n",
    "    print(f\"- Total Spans: {total_spans}\")\n",
    "    print(f\"- Avg Spans per Doc: {total_spans / total_docs:.2f}\")\n",
    "    print(f\"- Tag frequency: {first_level_counter.most_common()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: spancat\n",
      "- Optimize for: efficiency\n",
      "- Hardware: GPU\n",
      "- Transformer: roberta-base\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/home/zlovoblachko/diploma/spacy_training/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config /home/zlovoblachko/diploma/spacy_training/config.cfg --pipeline transformer,spancat --gpu --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: transformer, spancat\n",
      "6643 training docs\n",
      "1898 evaluation docs\n",
      "\u001b[38;5;3m⚠ 3 training examples also in evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 1371274 total word(s) in the data (17507 unique)\u001b[0m\n",
      "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "============================ Span Categorization ============================\u001b[0m\n",
      "\n",
      "Spans Key   Labels         \n",
      "---------   ---------------\n",
      "sc          {'U', 'R', 'M'}\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2Khuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2Khuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2Kalyzing label distribution...huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2K\u001b[38;5;4mℹ Span characteristics for spans_key 'sc'\u001b[0m\n",
      "\u001b[38;5;4mℹ SD = Span Distinctiveness, BD = Boundary Distinctiveness\u001b[0m\n",
      "\n",
      "Span Type      Length     SD     BD       N\n",
      "------------   ------   ----   ----   -----\n",
      "R                1.48   0.21   0.11   46006\n",
      "M                1.94   0.19   0.17   16796\n",
      "U                3.00   0.23   0.28    4373\n",
      "------------   ------   ----   ----   -----\n",
      "Wgt. Average     1.69   0.20   0.13       -\n",
      "\n",
      "\u001b[38;5;4mℹ Over 90% of spans have lengths of 1 -- 4 (min=1, max=37). The most\n",
      "common span lengths are: 1 (49.27%), 2 (27.46%), 3 (12.21%), 4 (5.28%). If you\n",
      "are using the n-gram suggester, note that omitting infrequent n-gram lengths can\n",
      "greatly improve speed and memory usage.\u001b[0m\n",
      "\u001b[38;5;3m⚠ Spans may not be distinct from the rest of the corpus\u001b[0m\n",
      "\u001b[38;5;3m⚠ Boundary tokens are not distinct from the rest of the corpus\u001b[0m\n",
      "\u001b[38;5;2m✔ Good amount of examples for all labels\u001b[0m\n",
      "\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m✔ 4 checks passed\u001b[0m\n",
      "\u001b[38;5;3m⚠ 3 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data /home/zlovoblachko/diploma/spacy_training/config.cfg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_realec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
