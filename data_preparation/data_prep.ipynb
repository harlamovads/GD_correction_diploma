{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_list(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        texts = pd.read_csv(f, sep='\\t', encoding='utf-16 LE')\n",
    "    return list(texts[texts['text_graph_desc'] == True]['text_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['/home/zlovoblachko/local_realec/Exam2014/Task 1/Exam2014_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2015/Task 1/Exam2015_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2016/Task 1/Exam2016_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2017/Task 1/Exam2017_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2019/Task 1/Exam2019_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 1/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 2/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 3/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 4/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 5/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 6/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 7/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 8/Exam2020_text_table.tsv']\n",
    "\n",
    "relevant_texts_nums = []\n",
    "\n",
    "for path in paths:\n",
    "    relevant_texts_nums.append(get_text_list(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_tables(file_path, texts_list):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        year_corpus = pd.read_csv(f, sep='\\t', encoding='utf-16 LE')\n",
    "    year_corpus['year'] = '_'.join(file_path.split('/')[5:7])\n",
    "    return year_corpus[year_corpus['text_id'].isin(texts_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_sentences = ['/home/zlovoblachko/diploma/data/Exam2014/Task 1/exam2014_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2015/Task 1/Exam2015_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2016/Task 1/exam2016_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2017/Task 1/Exam2017_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2019/Task 1/exam2019_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2020/Task 1/Exam2020_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2020/Task 2/Exam2020_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2020/Task 3/Exam2020_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2020/Task 4/Exam2020_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2020/Task 5/Exam2020_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2020/Task 6/Exam2020_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2020/Task 7/Exam2020_new_sentence_text_table.tsv',\n",
    "                  '/home/zlovoblachko/diploma/data/Exam2020/Task 8/Exam2020_new_sentence_text_table.tsv']\n",
    "\n",
    "all_sentences_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mistakes = \"/home/zlovoblachko/diploma/Labelled_dataset.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>sentence_poses</th>\n",
       "      <th>sentence_lemmas</th>\n",
       "      <th>sentence_token_spaces</th>\n",
       "      <th>sentence_orig_tokens</th>\n",
       "      <th>sentence_orig_token_spaces</th>\n",
       "      <th>sentence_token_deps</th>\n",
       "      <th>sentence_token_heads</th>\n",
       "      <th>sentence_token_spacy_poses</th>\n",
       "      <th>sentence_token_spacy_tags</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Given diagram visualize the proportion of popu...</td>\n",
       "      <td>AJ0 NN1 VVB AT0 NN1 PRF NN1 PRP CRD CJC AVP PR...</td>\n",
       "      <td>given|give diagram visualize the proportion of...</td>\n",
       "      <td>111111111111011111111101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep pobj ROOT det dobj prep pobj amod npadvmo...</td>\n",
       "      <td>2 0 2 4 2 4 5 6 7 8 8 4 11 12 12 14 18 18 14 4...</td>\n",
       "      <td>VERB NOUN VERB DET NOUN ADP NOUN VERB NUM CCON...</td>\n",
       "      <td>VBN NN VB DT NN IN NN VBN CD CC RB IN NNP , NN...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>In all of the countries , the proportion was g...</td>\n",
       "      <td>PRP DT0 PRF AT0 NN2 PUN AT0 NN1 VBD VVG AV0 PR...</td>\n",
       "      <td>in all of the country , the proportion be grow...</td>\n",
       "      <td>111101111111101111101111111101111011101101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep pobj prep det pobj punct det nsubj aux RO...</td>\n",
       "      <td>9 0 1 4 2 9 7 9 9 9 9 9 13 11 9 9 17 15 19 17 ...</td>\n",
       "      <td>ADP PRON ADP DET NOUN PUNCT DET NOUN AUX VERB ...</td>\n",
       "      <td>IN DT IN DT NNS , DT NN VBD VBG RB IN DT NN , ...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>After a stable period , we see a huge incline ...</td>\n",
       "      <td>PRP AT0 AJ0 NN1 PUN PNP VVB AT0 AJ0 NN1 CJT VM...</td>\n",
       "      <td>after a stable period , we see a huge incline ...</td>\n",
       "      <td>11101111111111111101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep det amod pobj punct nsubj ROOT det amod d...</td>\n",
       "      <td>6 3 3 0 6 6 6 9 9 6 12 12 9 12 13 12 15 15 17 6</td>\n",
       "      <td>ADP DET ADJ NOUN PUNCT PRON VERB DET ADJ NOUN ...</td>\n",
       "      <td>IN DT JJ NN , PRP VBP DT JJ NN WDT MD VB IN NN...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>About a third of a population would be aged 65...</td>\n",
       "      <td>PRP AT0 ORD PRF AT0 NN1 VM0 VBI PRP CRD CJC AV...</td>\n",
       "      <td>about a third of a population would be aged 65...</td>\n",
       "      <td>1111111111111011111011101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>advmod quantmod nsubj prep det pobj aux ROOT a...</td>\n",
       "      <td>2 2 7 2 5 3 7 7 7 8 9 9 7 12 7 7 15 15 19 20 2...</td>\n",
       "      <td>ADV DET NOUN ADP DET NOUN AUX AUX VERB NUM CCO...</td>\n",
       "      <td>RB DT NN IN DT NN MD VB VBN CD CC RB IN CD , V...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>USA , however , had bigger part of old people ...</td>\n",
       "      <td>NP0 PUN AV0 PUN VHD AJC NN1 PRF AJ0 NN0 PRP AT...</td>\n",
       "      <td>usa , however , have big part of old people th...</td>\n",
       "      <td>0101111111110110111110110111111111101101111101</td>\n",
       "      <td>USA , however , had bigger part of old people ...</td>\n",
       "      <td>0101111111110110111110110111111111101101111101</td>\n",
       "      <td>nsubj punct advmod punct ROOT amod dobj prep a...</td>\n",
       "      <td>4 4 4 4 4 6 4 6 9 7 4 12 10 4 4 16 14 14 17 14...</td>\n",
       "      <td>PROPN PUNCT ADV PUNCT VERB ADJ NOUN ADP ADJ NO...</td>\n",
       "      <td>NNP , RB , VBD JJR NN IN JJ NNS IN DT NN , VBG...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id  paragraph_id  sentence_id  \\\n",
       "0        1             1            1   \n",
       "1        1             2            2   \n",
       "2        1             3            3   \n",
       "3        1             3            4   \n",
       "4        1             4            5   \n",
       "\n",
       "                                     sentence_tokens  \\\n",
       "0  Given diagram visualize the proportion of popu...   \n",
       "1  In all of the countries , the proportion was g...   \n",
       "2  After a stable period , we see a huge incline ...   \n",
       "3  About a third of a population would be aged 65...   \n",
       "4  USA , however , had bigger part of old people ...   \n",
       "\n",
       "                                      sentence_poses  \\\n",
       "0  AJ0 NN1 VVB AT0 NN1 PRF NN1 PRP CRD CJC AVP PR...   \n",
       "1  PRP DT0 PRF AT0 NN2 PUN AT0 NN1 VBD VVG AV0 PR...   \n",
       "2  PRP AT0 AJ0 NN1 PUN PNP VVB AT0 AJ0 NN1 CJT VM...   \n",
       "3  PRP AT0 ORD PRF AT0 NN1 VM0 VBI PRP CRD CJC AV...   \n",
       "4  NP0 PUN AV0 PUN VHD AJC NN1 PRF AJ0 NN0 PRP AT...   \n",
       "\n",
       "                                     sentence_lemmas  \\\n",
       "0  given|give diagram visualize the proportion of...   \n",
       "1  in all of the country , the proportion be grow...   \n",
       "2  after a stable period , we see a huge incline ...   \n",
       "3  about a third of a population would be aged 65...   \n",
       "4  usa , however , have big part of old people th...   \n",
       "\n",
       "                            sentence_token_spaces  \\\n",
       "0                        111111111111011111111101   \n",
       "1      111101111111101111101111111101111011101101   \n",
       "2                            11101111111111111101   \n",
       "3                       1111111111111011111011101   \n",
       "4  0101111111110110111110110111111111101101111101   \n",
       "\n",
       "                                sentence_orig_tokens  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  USA , however , had bigger part of old people ...   \n",
       "\n",
       "                       sentence_orig_token_spaces  \\\n",
       "0                                             NaN   \n",
       "1                                             NaN   \n",
       "2                                             NaN   \n",
       "3                                             NaN   \n",
       "4  0101111111110110111110110111111111101101111101   \n",
       "\n",
       "                                 sentence_token_deps  \\\n",
       "0  prep pobj ROOT det dobj prep pobj amod npadvmo...   \n",
       "1  prep pobj prep det pobj punct det nsubj aux RO...   \n",
       "2  prep det amod pobj punct nsubj ROOT det amod d...   \n",
       "3  advmod quantmod nsubj prep det pobj aux ROOT a...   \n",
       "4  nsubj punct advmod punct ROOT amod dobj prep a...   \n",
       "\n",
       "                                sentence_token_heads  \\\n",
       "0  2 0 2 4 2 4 5 6 7 8 8 4 11 12 12 14 18 18 14 4...   \n",
       "1  9 0 1 4 2 9 7 9 9 9 9 9 13 11 9 9 17 15 19 17 ...   \n",
       "2    6 3 3 0 6 6 6 9 9 6 12 12 9 12 13 12 15 15 17 6   \n",
       "3  2 2 7 2 5 3 7 7 7 8 9 9 7 12 7 7 15 15 19 20 2...   \n",
       "4  4 4 4 4 4 6 4 6 9 7 4 12 10 4 4 16 14 14 17 14...   \n",
       "\n",
       "                          sentence_token_spacy_poses  \\\n",
       "0  VERB NOUN VERB DET NOUN ADP NOUN VERB NUM CCON...   \n",
       "1  ADP PRON ADP DET NOUN PUNCT DET NOUN AUX VERB ...   \n",
       "2  ADP DET ADJ NOUN PUNCT PRON VERB DET ADJ NOUN ...   \n",
       "3  ADV DET NOUN ADP DET NOUN AUX AUX VERB NUM CCO...   \n",
       "4  PROPN PUNCT ADV PUNCT VERB ADJ NOUN ADP ADJ NO...   \n",
       "\n",
       "                           sentence_token_spacy_tags             year  \n",
       "0  VBN NN VB DT NN IN NN VBN CD CC RB IN NNP , NN...  Exam2014_Task 1  \n",
       "1  IN DT IN DT NNS , DT NN VBD VBG RB IN DT NN , ...  Exam2014_Task 1  \n",
       "2  IN DT JJ NN , PRP VBP DT JJ NN WDT MD VB IN NN...  Exam2014_Task 1  \n",
       "3  RB DT NN IN DT NN MD VB VBN CD CC RB IN CD , V...  Exam2014_Task 1  \n",
       "4  NNP , RB , VBD JJR NN IN JJ NNS IN DT NN , VBG...  Exam2014_Task 1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for file_path, text_list in zip(paths_sentences, relevant_texts_nums):\n",
    "    all_sentences_df = pd.concat([all_sentences_df, get_sentences_tables(file_path, text_list)])\n",
    "\n",
    "all_sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_mistakes, \"rb\") as f:\n",
    "    mistakes_df = pd.read_csv(f, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>mistake_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>mistake_type</th>\n",
       "      <th>error_span</th>\n",
       "      <th>correction</th>\n",
       "      <th>span_start</th>\n",
       "      <th>span_end</th>\n",
       "      <th>year</th>\n",
       "      <th>bigger_code</th>\n",
       "      <th>first_level_tag</th>\n",
       "      <th>second_level_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Absence_explanation</td>\n",
       "      <td>century</td>\n",
       "      <td>XXth century</td>\n",
       "      <td>200</td>\n",
       "      <td>207</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "      <td>Discourse</td>\n",
       "      <td>M</td>\n",
       "      <td>MULTIWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Absence_explanation</td>\n",
       "      <td>a population</td>\n",
       "      <td>the Japanese population</td>\n",
       "      <td>436</td>\n",
       "      <td>448</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "      <td>Discourse</td>\n",
       "      <td>R</td>\n",
       "      <td>MULTIWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tense_choice</td>\n",
       "      <td>would be</td>\n",
       "      <td>is going to be</td>\n",
       "      <td>449</td>\n",
       "      <td>457</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "      <td>Grammar</td>\n",
       "      <td>R</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>,</td>\n",
       "      <td>-</td>\n",
       "      <td>482</td>\n",
       "      <td>483</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>R</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Absence_explanation</td>\n",
       "      <td>century</td>\n",
       "      <td>XXth century</td>\n",
       "      <td>583</td>\n",
       "      <td>590</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "      <td>Discourse</td>\n",
       "      <td>M</td>\n",
       "      <td>MULTIWORD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  mistake_id  text_id  sentence_id         mistake_type  \\\n",
       "0        4      7           8        1            2  Absence_explanation   \n",
       "1        6     10          11        1            4  Absence_explanation   \n",
       "2        7     11          12        1            4         Tense_choice   \n",
       "3        8     12          13        1            4          Punctuation   \n",
       "4       10     16          17        1            5  Absence_explanation   \n",
       "\n",
       "     error_span               correction  span_start  span_end  \\\n",
       "0       century             XXth century         200       207   \n",
       "1  a population  the Japanese population         436       448   \n",
       "2      would be           is going to be         449       457   \n",
       "3             ,                        -         482       483   \n",
       "4       century             XXth century         583       590   \n",
       "\n",
       "              year  bigger_code first_level_tag second_level_tag  \n",
       "0  Exam2014_Task 1    Discourse               M        MULTIWORD  \n",
       "1  Exam2014_Task 1    Discourse               R        MULTIWORD  \n",
       "2  Exam2014_Task 1      Grammar               R             VERB  \n",
       "3  Exam2014_Task 1  Punctuation               R            PUNCT  \n",
       "4  Exam2014_Task 1    Discourse               M        MULTIWORD  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence_tokens</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Given diagram visualize the proportion of popu...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>In all of the countries , the proportion was g...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>After a stable period , we see a huge incline ...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>About a third of a population would be aged 65...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>USA , however , had bigger part of old people ...</td>\n",
       "      <td>Exam2014_Task 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id  paragraph_id  sentence_id  \\\n",
       "0        1             1            1   \n",
       "1        1             2            2   \n",
       "2        1             3            3   \n",
       "3        1             3            4   \n",
       "4        1             4            5   \n",
       "\n",
       "                                     sentence_tokens             year  \n",
       "0  Given diagram visualize the proportion of popu...  Exam2014_Task 1  \n",
       "1  In all of the countries , the proportion was g...  Exam2014_Task 1  \n",
       "2  After a stable period , we see a huge incline ...  Exam2014_Task 1  \n",
       "3  About a third of a population would be aged 65...  Exam2014_Task 1  \n",
       "4  USA , however , had bigger part of old people ...  Exam2014_Task 1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences_filtered_df = all_sentences_df[['text_id', 'paragraph_id', 'sentence_id', 'sentence_tokens', 'year']]\n",
    "all_sentences_filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1675/3529073697.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_sentences_filtered_df.sort_values(by=[\"year\", \"text_id\", \"sentence_id\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "all_sentences_filtered_df.sort_values(by=[\"year\", \"text_id\", \"sentence_id\"], inplace=True)\n",
    "mistakes_df.sort_values(by=[\"year\", \"text_id\", \"sentence_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, sentence_row in all_sentences_filtered_df.iterrows():\n",
    "    text_id = sentence_row[\"text_id\"]\n",
    "    sentence_id = sentence_row[\"sentence_id\"]\n",
    "    sentence = sentence_row[\"sentence_tokens\"]\n",
    "    mistakes_in_sentence = mistakes_df[(mistakes_df[\"text_id\"] == text_id) & (mistakes_df[\"sentence_id\"] == sentence_id)]\n",
    "    annotations = []\n",
    "    for _, mistake_row in mistakes_in_sentence.iterrows():\n",
    "        span_start = mistake_row[\"span_start\"]\n",
    "        span_end = mistake_row[\"span_end\"]\n",
    "        annotations.append({\n",
    "            \"start\": span_start,\n",
    "            \"end\": span_end,\n",
    "            \"first_level_tag\": str(mistake_row[\"first_level_tag\"]).strip(),\n",
    "            \"second_level_tag\": str(mistake_row[\"second_level_tag\"]).strip(),\n",
    "            \"correction\": mistake_row[\"correction\"]\n",
    "        })\n",
    "    training_data.append({\"text\": sentence, \"annotations\": annotations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Final_training_data.jsonl with 85066 entries.\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"Final_training_data.jsonl\"\n",
    "\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in training_data:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Generated {output_filename} with {len(training_data)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "jsonl_file = \"Final_training_data.jsonl\"\n",
    "output_spacy_file = \"Final_training_data.spacy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = [json.loads(line) for line in f]\n",
    "\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in training_data:\n",
    "    text = entry[\"text\"]\n",
    "    annotations = entry[\"annotations\"]\n",
    "    doc = nlp.make_doc(text)\n",
    "    spans = []\n",
    "    for ann in annotations:\n",
    "        start, end = ann[\"start\"], ann[\"end\"]\n",
    "        first_level_tag = ann[\"first_level_tag\"]\n",
    "        second_level_tag = ann[\"second_level_tag\"]\n",
    "        label = f\"{first_level_tag}-{second_level_tag}\"\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span:\n",
    "            spans.append(span)\n",
    "    doc.spans[\"sc\"] = spans\n",
    "    doc_bin.add(doc)\n",
    "doc_bin.to_disk(output_spacy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_disk(output_spacy_file)\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "dev_ratio = 0.2\n",
    "test_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = len(docs)\n",
    "train_end = int(total_docs * train_ratio)\n",
    "dev_end = train_end + int(total_docs * dev_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = docs[:train_end]\n",
    "dev_docs = docs[train_end:dev_end]\n",
    "test_docs = docs[dev_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = {\n",
    "    \"train.spacy\": train_docs,\n",
    "    \"dev.spacy\": dev_docs,\n",
    "    \"test.spacy\": test_docs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, doc_subset in output_files.items():\n",
    "    doc_bin_subset = DocBin()\n",
    "    for doc in doc_subset:\n",
    "        doc_bin_subset.add(doc)\n",
    "    doc_bin_subset.to_disk(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_realec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
