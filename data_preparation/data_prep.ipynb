{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_list(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        texts = pd.read_csv(f, sep='\\t', encoding='utf-16 LE')\n",
    "    filtered_texts = texts.loc[texts['text_graph_desc'] == True, ['text_name', 'text_year', 'text_id']].copy()\n",
    "    filtered_texts['year'] = filtered_texts['text_year']\n",
    "    if 2021 in filtered_texts['year'].unique():\n",
    "        print(file_path)\n",
    "    return filtered_texts.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zlovoblachko/local_realec/Exam2020/Task 1/Exam2020_text_table.tsv\n"
     ]
    }
   ],
   "source": [
    "paths = ['/home/zlovoblachko/local_realec/Exam2014/Task 1/Exam2014_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2015/Task 1/Exam2015_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2016/Task 1/Exam2016_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2017/Task 1/Exam2017_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2019/Task 1/Exam2019_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 1/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 2/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 3/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 4/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 5/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 6/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 7/Exam2020_text_table.tsv',\n",
    "         '/home/zlovoblachko/local_realec/Exam2020/Task 8/Exam2020_text_table.tsv']\n",
    "\n",
    "relevant_texts_nums = pd.DataFrame()\n",
    "\n",
    "for path in paths:\n",
    "    relevant_texts_nums = pd.concat([relevant_texts_nums, get_text_list(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_name</th>\n",
       "      <th>text_year</th>\n",
       "      <th>text_id</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014_AAl_10_1.txt</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014_AAl_11_1.txt</td>\n",
       "      <td>2014</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014_AAl_12_1.txt</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014_AAl_13_1.txt</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014_AAl_14_1.txt</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2020_MLa_6404_1.txt</td>\n",
       "      <td>2020</td>\n",
       "      <td>447</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2020_MLa_6405_1.txt</td>\n",
       "      <td>2020</td>\n",
       "      <td>449</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2020_MLa_6406_1.txt</td>\n",
       "      <td>2020</td>\n",
       "      <td>451</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2020_MLa_6407_1.txt</td>\n",
       "      <td>2020</td>\n",
       "      <td>453</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2020_MLa_6408_1.txt</td>\n",
       "      <td>2020</td>\n",
       "      <td>455</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9490 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               text_name  text_year  text_id  year\n",
       "0      2014_AAl_10_1.txt       2014        1  2014\n",
       "1      2014_AAl_11_1.txt       2014        3  2014\n",
       "2      2014_AAl_12_1.txt       2014        5  2014\n",
       "3      2014_AAl_13_1.txt       2014        7  2014\n",
       "4      2014_AAl_14_1.txt       2014        9  2014\n",
       "..                   ...        ...      ...   ...\n",
       "224  2020_MLa_6404_1.txt       2020      447  2020\n",
       "225  2020_MLa_6405_1.txt       2020      449  2020\n",
       "226  2020_MLa_6406_1.txt       2020      451  2020\n",
       "227  2020_MLa_6407_1.txt       2020      453  2020\n",
       "228  2020_MLa_6408_1.txt       2020      455  2020\n",
       "\n",
       "[9490 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_texts_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mistakes = \"/home/zlovoblachko/diploma/Labelled_dataset.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_mistakes, \"rb\") as f:\n",
    "    mistakes_df = pd.read_csv(f, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_df['year'] = mistakes_df['year'].apply(lambda row: row.split('_')[0][4:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_df['year'] = mistakes_df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>mistake_id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>mistake_type</th>\n",
       "      <th>error_span</th>\n",
       "      <th>correction</th>\n",
       "      <th>span_start</th>\n",
       "      <th>span_end</th>\n",
       "      <th>year</th>\n",
       "      <th>bigger_code</th>\n",
       "      <th>first_level_tag</th>\n",
       "      <th>second_level_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Absence_explanation</td>\n",
       "      <td>century</td>\n",
       "      <td>XXth century</td>\n",
       "      <td>200</td>\n",
       "      <td>207</td>\n",
       "      <td>2014</td>\n",
       "      <td>Discourse</td>\n",
       "      <td>M</td>\n",
       "      <td>MULTIWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Absence_explanation</td>\n",
       "      <td>a population</td>\n",
       "      <td>the Japanese population</td>\n",
       "      <td>436</td>\n",
       "      <td>448</td>\n",
       "      <td>2014</td>\n",
       "      <td>Discourse</td>\n",
       "      <td>R</td>\n",
       "      <td>MULTIWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Tense_choice</td>\n",
       "      <td>would be</td>\n",
       "      <td>is going to be</td>\n",
       "      <td>449</td>\n",
       "      <td>457</td>\n",
       "      <td>2014</td>\n",
       "      <td>Grammar</td>\n",
       "      <td>R</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>,</td>\n",
       "      <td>-</td>\n",
       "      <td>482</td>\n",
       "      <td>483</td>\n",
       "      <td>2014</td>\n",
       "      <td>Punctuation</td>\n",
       "      <td>R</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Absence_explanation</td>\n",
       "      <td>century</td>\n",
       "      <td>XXth century</td>\n",
       "      <td>583</td>\n",
       "      <td>590</td>\n",
       "      <td>2014</td>\n",
       "      <td>Discourse</td>\n",
       "      <td>M</td>\n",
       "      <td>MULTIWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Tense_choice</td>\n",
       "      <td>would be</td>\n",
       "      <td>is expected to be</td>\n",
       "      <td>654</td>\n",
       "      <td>662</td>\n",
       "      <td>2014</td>\n",
       "      <td>Grammar</td>\n",
       "      <td>R</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Tense_choice</td>\n",
       "      <td>would be</td>\n",
       "      <td>are going to be</td>\n",
       "      <td>696</td>\n",
       "      <td>704</td>\n",
       "      <td>2014</td>\n",
       "      <td>Grammar</td>\n",
       "      <td>R</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Word_choice</td>\n",
       "      <td>Sweden chart</td>\n",
       "      <td>The graph for Sweden</td>\n",
       "      <td>720</td>\n",
       "      <td>732</td>\n",
       "      <td>2014</td>\n",
       "      <td>Vocabulary</td>\n",
       "      <td>M</td>\n",
       "      <td>MULTIWORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Ref_device</td>\n",
       "      <td>the US one</td>\n",
       "      <td>that of the USA</td>\n",
       "      <td>752</td>\n",
       "      <td>762</td>\n",
       "      <td>2014</td>\n",
       "      <td>Discourse</td>\n",
       "      <td>M</td>\n",
       "      <td>WORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Formational_affixes</td>\n",
       "      <td>tendention</td>\n",
       "      <td>tendency</td>\n",
       "      <td>868</td>\n",
       "      <td>878</td>\n",
       "      <td>2014</td>\n",
       "      <td>Vocabulary</td>\n",
       "      <td>R</td>\n",
       "      <td>FORM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  mistake_id  text_id  sentence_id         mistake_type  \\\n",
       "0        4      7           8        1            2  Absence_explanation   \n",
       "1        6     10          11        1            4  Absence_explanation   \n",
       "2        7     11          12        1            4         Tense_choice   \n",
       "3        8     12          13        1            4          Punctuation   \n",
       "4       10     16          17        1            5  Absence_explanation   \n",
       "5       12     19          20        1            5         Tense_choice   \n",
       "6       13     20          21        1            5         Tense_choice   \n",
       "7       14     21          22        1            6          Word_choice   \n",
       "8       15     22          23        1            6           Ref_device   \n",
       "9       17     24          25        1            7  Formational_affixes   \n",
       "\n",
       "     error_span               correction  span_start  span_end  year  \\\n",
       "0       century             XXth century         200       207  2014   \n",
       "1  a population  the Japanese population         436       448  2014   \n",
       "2      would be           is going to be         449       457  2014   \n",
       "3             ,                        -         482       483  2014   \n",
       "4       century             XXth century         583       590  2014   \n",
       "5      would be        is expected to be         654       662  2014   \n",
       "6      would be          are going to be         696       704  2014   \n",
       "7  Sweden chart     The graph for Sweden         720       732  2014   \n",
       "8    the US one          that of the USA         752       762  2014   \n",
       "9    tendention                 tendency         868       878  2014   \n",
       "\n",
       "   bigger_code first_level_tag second_level_tag  \n",
       "0    Discourse               M        MULTIWORD  \n",
       "1    Discourse               R        MULTIWORD  \n",
       "2      Grammar               R             VERB  \n",
       "3  Punctuation               R            PUNCT  \n",
       "4    Discourse               M        MULTIWORD  \n",
       "5      Grammar               R             VERB  \n",
       "6      Grammar               R             VERB  \n",
       "7   Vocabulary               M        MULTIWORD  \n",
       "8    Discourse               M             WORD  \n",
       "9   Vocabulary               R             FORM  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45076"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mistakes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_df['span_start'] = mistakes_df['span_start'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_name</th>\n",
       "      <th>text_year</th>\n",
       "      <th>text_id</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>2021_MLa_100_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>1820</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>2021_MLa_101_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>1822</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>2021_MLa_102_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>1824</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>2021_MLa_10_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>1826</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>2021_MLa_11_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>1828</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>2021_MLa_96_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>2014</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>2021_MLa_97_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>2016</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>2021_MLa_98_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>2018</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>2021_MLa_99_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>2020</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>2021_MLa_9_1.txt</td>\n",
       "      <td>2021</td>\n",
       "      <td>2022</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               text_name  text_year  text_id  year\n",
       "909   2021_MLa_100_1.txt       2021     1820  2021\n",
       "910   2021_MLa_101_1.txt       2021     1822  2021\n",
       "911   2021_MLa_102_1.txt       2021     1824  2021\n",
       "912    2021_MLa_10_1.txt       2021     1826  2021\n",
       "913    2021_MLa_11_1.txt       2021     1828  2021\n",
       "...                  ...        ...      ...   ...\n",
       "1006   2021_MLa_96_1.txt       2021     2014  2021\n",
       "1007   2021_MLa_97_1.txt       2021     2016  2021\n",
       "1008   2021_MLa_98_1.txt       2021     2018  2021\n",
       "1009   2021_MLa_99_1.txt       2021     2020  2021\n",
       "1010    2021_MLa_9_1.txt       2021     2022  2021\n",
       "\n",
       "[102 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_texts_nums[relevant_texts_nums['year'] == 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2019\n",
      "2020\n",
      "2021\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "mistake_counter = 0\n",
    "ann_len = 0\n",
    "for year in relevant_texts_nums['year'].unique():\n",
    "    print(year)\n",
    "    year_text_df = relevant_texts_nums[relevant_texts_nums['year'] == year]\n",
    "    year_mistakes_df = mistakes_df[mistakes_df['year'] == year]\n",
    "    for _, text_row in year_text_df.iterrows():\n",
    "        text_mistakes_df = year_mistakes_df[year_mistakes_df['text_id'] == text_row['text_id']]\n",
    "        text_mistakes_df = text_mistakes_df.sort_values('span_start')\n",
    "        try:\n",
    "            with open('/home/zlovoblachko/diploma/data/rawfiles/' + text_row['text_name']) as f:\n",
    "                text = f.read()\n",
    "        except FileNotFoundError:\n",
    "            ## print(text_row['text_name'])\n",
    "            pass\n",
    "        annotations = []\n",
    "        for _, mistake_row in text_mistakes_df.iterrows():\n",
    "                annotations.append({\n",
    "                    \"start\": mistake_row['span_start'],\n",
    "                    \"end\": mistake_row['span_end'],\n",
    "                    \"error_span\": text[mistake_row['span_start']:mistake_row['span_end']],\n",
    "                    \"first_level_tag\": str(mistake_row[\"first_level_tag\"]),\n",
    "                    \"second_level_tag\": str(mistake_row[\"second_level_tag\"]),\n",
    "                    \"correction\": mistake_row[\"correction\"]\n",
    "                })\n",
    "                mistake_counter += 1\n",
    "        training_data.append({\"text\": text, \"annotations\": annotations})\n",
    "        ann_len += len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100939\n",
      "100939\n"
     ]
    }
   ],
   "source": [
    "print(mistake_counter)\n",
    "print(ann_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Given diagram visualize the proportion of population aged 65 and over in Japan, Sweden and the United States from 1940 to 2040. \\nIn all of the countries, the proportion was growing rapidly during the century, with Japan being an exception, where it was stable from 1940 to 1980, decreasing slightly from 5% to about 3% in 1960. \\nAfter a stable period, we see a huge incline that would occur in Japan from 2020 to 2040. About a third of a population would be aged 65 and over in 2040, compare that to below 5% part in 2000. \\nUSA, however, had bigger part of old people throughout the century, having 10% in 1960 and even 15% in 1980, but the overall part would be below Japanese in 2040, whew 25% would be 65 and older. \\nSweden chart is almost equal to the US one, only big discrepancy is 20% in 2020 in Sweden versus below 15% in the US. \\nOverall, we can see a strong tendention that population is getting older at a whole with time in these countries.',\n",
       " 'annotations': [{'start': 0,\n",
       "   'end': 13,\n",
       "   'error_span': 'Given diagram',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'DET',\n",
       "   'correction': 'The given diagram'},\n",
       "  {'start': 42,\n",
       "   'end': 52,\n",
       "   'error_span': 'population',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'DET',\n",
       "   'correction': 'the population'},\n",
       "  {'start': 200,\n",
       "   'end': 207,\n",
       "   'error_span': 'century',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'MULTIWORD',\n",
       "   'correction': 'XXth century'},\n",
       "  {'start': 436,\n",
       "   'end': 448,\n",
       "   'error_span': 'a population',\n",
       "   'first_level_tag': 'R',\n",
       "   'second_level_tag': 'MULTIWORD',\n",
       "   'correction': 'the Japanese population'},\n",
       "  {'start': 449,\n",
       "   'end': 457,\n",
       "   'error_span': 'would be',\n",
       "   'first_level_tag': 'R',\n",
       "   'second_level_tag': 'VERB',\n",
       "   'correction': 'is going to be'},\n",
       "  {'start': 482,\n",
       "   'end': 483,\n",
       "   'error_span': ',',\n",
       "   'first_level_tag': 'R',\n",
       "   'second_level_tag': 'PUNCT',\n",
       "   'correction': '-'},\n",
       "  {'start': 524,\n",
       "   'end': 527,\n",
       "   'error_span': 'USA',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'DET',\n",
       "   'correction': 'The USA'},\n",
       "  {'start': 583,\n",
       "   'end': 590,\n",
       "   'error_span': 'century',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'MULTIWORD',\n",
       "   'correction': 'XXth century'},\n",
       "  {'start': 654,\n",
       "   'end': 662,\n",
       "   'error_span': 'would be',\n",
       "   'first_level_tag': 'R',\n",
       "   'second_level_tag': 'VERB',\n",
       "   'correction': 'is expected to be'},\n",
       "  {'start': 669,\n",
       "   'end': 677,\n",
       "   'error_span': 'Japanese',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'WORD',\n",
       "   'correction': 'that of the Japanese'},\n",
       "  {'start': 696,\n",
       "   'end': 704,\n",
       "   'error_span': 'would be',\n",
       "   'first_level_tag': 'R',\n",
       "   'second_level_tag': 'VERB',\n",
       "   'correction': 'are going to be'},\n",
       "  {'start': 720,\n",
       "   'end': 732,\n",
       "   'error_span': 'Sweden chart',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'MULTIWORD',\n",
       "   'correction': 'The graph for Sweden'},\n",
       "  {'start': 752,\n",
       "   'end': 762,\n",
       "   'error_span': 'the US one',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'WORD',\n",
       "   'correction': 'that of the USA'},\n",
       "  {'start': 868,\n",
       "   'end': 878,\n",
       "   'error_span': 'tendention',\n",
       "   'first_level_tag': 'R',\n",
       "   'second_level_tag': 'FORM',\n",
       "   'correction': 'tendency'},\n",
       "  {'start': 884,\n",
       "   'end': 894,\n",
       "   'error_span': 'population',\n",
       "   'first_level_tag': 'M',\n",
       "   'second_level_tag': 'DET',\n",
       "   'correction': 'the population'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the first glance we can notice that men are more active users of Facebook than women. Only in receiving updates percentage of men is 30 percent and it equals to percentage of women. As we can see women use Facebook on the mobile phones more often than men but not as often as on desktop. However women use facebook on their leptops an computers rarely than men. The number of the quantity of men using Facebook on desktops devided by the same quantity of women is about 6/7. This number is definately less than the number of the quantity of women using Facebook on their phones devided by the same quantity of men. That means that women like to use Facebook mobile version while they have been doing other buisenesses.\n",
      "Men more often sharing video on about 15 percent. Also men watch funny videos more often.\n",
      "So i think that such diagrams demonstrate us that men use Facebook previously for joy. Women use Facebook for an entertainment rarely but more often use it for a work.\n",
      "3 19\n",
      "the first glance\n",
      "309 317\n",
      "facebook\n",
      "815 816\n",
      "i\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "entry = training_data[-5675]\n",
    "print(entry['text'])\n",
    "for anns in entry['annotations']:\n",
    "    print(anns['start'], anns['end'])\n",
    "    print(entry['text'][anns['start']:anns['end']])\n",
    "print('---' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Textwise_training_data.jsonl with 9490 entries.\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"Textwise_training_data.jsonl\"\n",
    "\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in training_data:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Generated {output_filename} with {len(training_data)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/zlovoblachko/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021_MLa_100_1.txt\n",
      "2021_MLa_101_1.txt\n",
      "2021_MLa_102_1.txt\n",
      "2021_MLa_10_1.txt\n",
      "2021_MLa_11_1.txt\n",
      "2021_MLa_12_1.txt\n",
      "2021_MLa_13_1.txt\n",
      "2021_MLa_14_1.txt\n",
      "2021_MLa_15_1.txt\n",
      "2021_MLa_16_1.txt\n",
      "2021_MLa_17_1.txt\n",
      "2021_MLa_18_1.txt\n",
      "2021_MLa_19_1.txt\n",
      "2021_MLa_1_1.txt\n",
      "2021_MLa_20_1.txt\n",
      "2021_MLa_21_1.txt\n",
      "2021_MLa_22_1.txt\n",
      "2021_MLa_23_1.txt\n",
      "2021_MLa_24_1.txt\n",
      "2021_MLa_25_1.txt\n",
      "2021_MLa_26_1.txt\n",
      "2021_MLa_27_1.txt\n",
      "2021_MLa_28_1.txt\n",
      "2021_MLa_29_1.txt\n",
      "2021_MLa_2_1.txt\n",
      "2021_MLa_30_1.txt\n",
      "2021_MLa_31_1.txt\n",
      "2021_MLa_32_1.txt\n",
      "2021_MLa_33_1.txt\n",
      "2021_MLa_34_1.txt\n",
      "2021_MLa_35_1.txt\n",
      "2021_MLa_36_1.txt\n",
      "2021_MLa_37_1.txt\n",
      "2021_MLa_38_1.txt\n",
      "2021_MLa_39_1.txt\n",
      "2021_MLa_3_1.txt\n",
      "2021_MLa_40_1.txt\n",
      "2021_MLa_41_1.txt\n",
      "2021_MLa_42_1.txt\n",
      "2021_MLa_43_1.txt\n",
      "2021_MLa_44_1.txt\n",
      "2021_MLa_45_1.txt\n",
      "2021_MLa_46_1.txt\n",
      "2021_MLa_47_1.txt\n",
      "2021_MLa_48_1.txt\n",
      "2021_MLa_49_1.txt\n",
      "2021_MLa_4_1.txt\n",
      "2021_MLa_50_1.txt\n",
      "2021_MLa_51_1.txt\n",
      "2021_MLa_52_1.txt\n",
      "2021_MLa_53_1.txt\n",
      "2021_MLa_54_1.txt\n",
      "2021_MLa_55_1.txt\n",
      "2021_MLa_56_1.txt\n",
      "2021_MLa_57_1.txt\n",
      "2021_MLa_58_1.txt\n",
      "2021_MLa_59_1.txt\n",
      "2021_MLa_5_1.txt\n",
      "2021_MLa_60_1.txt\n",
      "2021_MLa_61_1.txt\n",
      "2021_MLa_62_1.txt\n",
      "2021_MLa_63_1.txt\n",
      "2021_MLa_64_1.txt\n",
      "2021_MLa_65_1.txt\n",
      "2021_MLa_66_1.txt\n",
      "2021_MLa_67_1.txt\n",
      "2021_MLa_68_1.txt\n",
      "2021_MLa_69_1.txt\n",
      "2021_MLa_6_1.txt\n",
      "2021_MLa_70_1.txt\n",
      "2021_MLa_71_1.txt\n",
      "2021_MLa_72_1.txt\n",
      "2021_MLa_73_1.txt\n",
      "2021_MLa_74_1.txt\n",
      "2021_MLa_75_1.txt\n",
      "2021_MLa_76_1.txt\n",
      "2021_MLa_77_1.txt\n",
      "2021_MLa_78_1.txt\n",
      "2021_MLa_79_1.txt\n",
      "2021_MLa_7_1.txt\n",
      "2021_MLa_80_1.txt\n",
      "2021_MLa_81_1.txt\n",
      "2021_MLa_82_1.txt\n",
      "2021_MLa_83_1.txt\n",
      "2021_MLa_84_1.txt\n",
      "2021_MLa_85_1.txt\n",
      "2021_MLa_86_1.txt\n",
      "2021_MLa_87_1.txt\n",
      "2021_MLa_88_1.txt\n",
      "2021_MLa_89_1.txt\n",
      "2021_MLa_8_1.txt\n",
      "2021_MLa_90_1.txt\n",
      "2021_MLa_91_1.txt\n",
      "2021_MLa_92_1.txt\n",
      "2021_MLa_93_1.txt\n",
      "2021_MLa_94_1.txt\n",
      "2021_MLa_95_1.txt\n",
      "2021_MLa_96_1.txt\n",
      "2021_MLa_97_1.txt\n",
      "2021_MLa_98_1.txt\n",
      "2021_MLa_99_1.txt\n",
      "2021_MLa_9_1.txt\n"
     ]
    }
   ],
   "source": [
    "sent_training_data = []\n",
    "mistake_counter_sent = 0\n",
    "ann_len = 0\n",
    "for year in relevant_texts_nums['year'].unique():\n",
    "    year_text_df = relevant_texts_nums[relevant_texts_nums['year'] == year]\n",
    "    year_mistakes_df = mistakes_df[mistakes_df['year'] == year]\n",
    "    for _, text_row in year_text_df.iterrows():\n",
    "        text_mistakes_df = year_mistakes_df[year_mistakes_df['text_id'] == text_row['text_id']]\n",
    "        text_mistakes_df = text_mistakes_df.sort_values('span_start')\n",
    "        try:\n",
    "            with open('/home/zlovoblachko/diploma/data/rawfiles/' + text_row['text_name']) as f:\n",
    "                text = f.read()\n",
    "                sentence_list = tokenizer.tokenize(text)\n",
    "        except FileNotFoundError:\n",
    "            print(text_row['text_name'])\n",
    "        current_symcount_before = 0\n",
    "        current_symcount_after = 0\n",
    "        for sentence in sentence_list:\n",
    "            current_symcount_after += len(sentence)\n",
    "            annotations = []\n",
    "            for _, mistake_row in text_mistakes_df.iterrows():\n",
    "                    if current_symcount_before <= mistake_row['span_start'] <= current_symcount_after:\n",
    "                        annotations.append({\n",
    "                            \"start\": mistake_row['span_start'] - current_symcount_before,\n",
    "                            \"end\": mistake_row['span_end'] - current_symcount_before,\n",
    "                            \"error_span\": text[mistake_row['span_start'] - current_symcount_before:mistake_row['span_end'] - current_symcount_before],\n",
    "                            \"first_level_tag\": str(mistake_row[\"first_level_tag\"]),\n",
    "                            \"second_level_tag\": str(mistake_row[\"second_level_tag\"]),\n",
    "                            \"correction\": mistake_row[\"correction\"]\n",
    "                        })\n",
    "                        mistake_counter_sent += 1\n",
    "            current_symcount_before += len(sentence)\n",
    "            if len(annotations) > 0:\n",
    "                sent_training_data.append({\"text\": sentence, \"annotations\": annotations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96397"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistake_counter_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Tokyo's underground system is standing out as the one that transports the greatest amount of passengers per year (1927 millions).\",\n",
       " 'annotations': [{'start': 124,\n",
       "   'end': 132,\n",
       "   'error_span': 'le that ',\n",
       "   'first_level_tag': 'R',\n",
       "   'second_level_tag': 'NUM',\n",
       "   'correction': 'million'}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_training_data[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentencewise_training_data.jsonl with 45347 entries.\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"Sentencewise_training_data.jsonl\"\n",
    "\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in sent_training_data:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Generated {output_filename} with {len(sent_training_data)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "jsonl_file = \"Sentencewise_training_data.jsonl\"\n",
    "output_spacy_file = \"Sentencewise_training_data.spacy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    training_data = [json.loads(line) for line in f]\n",
    "\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96397\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for entry in training_data:\n",
    "    if entry['annotations']:\n",
    "        for mistake in entry['annotations']:\n",
    "             if mistake['first_level_tag']:\n",
    "                 counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for entry in training_data:\n",
    "    text = entry[\"text\"]\n",
    "    annotations = entry[\"annotations\"]\n",
    "    doc = nlp.make_doc(text)\n",
    "    spans = []\n",
    "    for ann in annotations:\n",
    "        start, end = ann[\"start\"], ann[\"end\"]\n",
    "        label = ann[\"first_level_tag\"]\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"expand\")\n",
    "        if span:\n",
    "            spans.append(span)\n",
    "            counter += 1\n",
    "    doc.spans[\"sc\"] = spans\n",
    "    doc_bin.add(doc)\n",
    "doc_bin.to_disk(output_spacy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89192"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_disk(output_spacy_file)\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "dev_ratio = 0.2\n",
    "test_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = len(docs)\n",
    "train_end = int(total_docs * train_ratio)\n",
    "dev_end = train_end + int(total_docs * dev_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = docs[:train_end]\n",
    "dev_docs = docs[train_end:dev_end]\n",
    "test_docs = docs[dev_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = {\n",
    "    \"train.spacy\": train_docs,\n",
    "    \"dev.spacy\": dev_docs,\n",
    "    \"test.spacy\": test_docs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename, doc_subset in output_files.items():\n",
    "    doc_bin_subset = DocBin()\n",
    "    for doc in doc_subset:\n",
    "        doc_bin_subset.add(doc)\n",
    "    doc_bin_subset.to_disk(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels(data: dict):\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    for x, y in data.items():\n",
    "        labels.append(x)\n",
    "        sizes.append(y)\n",
    "    plt.pie(sizes, labels=labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.spacy:\n",
      "- Total Documents: 31742\n",
      "- Total Tokens: 766308\n",
      "- Avg Tokens per Doc: 24.14\n",
      "- Total Spans: 62518\n",
      "- Avg Spans per Doc: 1.97\n",
      "- Tag frequency: [('R', 43418), ('M', 15250), ('U', 3850)]\n",
      "--------------------------------------------------\n",
      "dev.spacy:\n",
      "- Total Documents: 9069\n",
      "- Total Tokens: 218357\n",
      "- Avg Tokens per Doc: 24.08\n",
      "- Total Spans: 17832\n",
      "- Avg Spans per Doc: 1.97\n",
      "- Tag frequency: [('R', 12374), ('M', 4393), ('U', 1065)]\n",
      "--------------------------------------------------\n",
      "test.spacy:\n",
      "- Total Documents: 4536\n",
      "- Total Tokens: 109640\n",
      "- Avg Tokens per Doc: 24.17\n",
      "- Total Spans: 8842\n",
      "- Avg Spans per Doc: 1.95\n",
      "- Tag frequency: [('R', 6068), ('M', 2190), ('U', 584)]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for filename in output_files.keys():\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc_bin = DocBin().from_disk(filename)\n",
    "    docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "    total_docs = len(docs)\n",
    "    total_tokens = sum(len(doc) for doc in docs)\n",
    "    total_spans = sum(len(doc.spans.get(\"sc\", [])) for doc in docs)\n",
    "    first_level_counter = Counter()\n",
    "    second_level_counter = Counter()\n",
    "    for doc in docs:\n",
    "        for span in doc.spans.get(\"sc\", []):\n",
    "            label = span.label_\n",
    "            first_level_counter[label] += 1\n",
    "    print(f\"{filename}:\")\n",
    "    print(f\"- Total Documents: {total_docs}\")\n",
    "    print(f\"- Total Tokens: {total_tokens}\")\n",
    "    print(f\"- Avg Tokens per Doc: {total_tokens / total_docs:.2f}\")\n",
    "    print(f\"- Total Spans: {total_spans}\")\n",
    "    print(f\"- Avg Spans per Doc: {total_spans / total_docs:.2f}\")\n",
    "    print(f\"- Tag frequency: {first_level_counter.most_common()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: spancat\n",
      "- Optimize for: efficiency\n",
      "- Hardware: GPU\n",
      "- Transformer: roberta-base\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/home/zlovoblachko/diploma/spacy_training/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config /home/zlovoblachko/diploma/spacy_training/config.cfg --pipeline transformer,spancat --gpu --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================ Data file validation ============================\u001b[0m\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
      "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
      "\u001b[1m\n",
      "=============================== Training stats ===============================\u001b[0m\n",
      "Language: en\n",
      "Training pipeline: transformer, spancat\n",
      "6643 training docs\n",
      "1898 evaluation docs\n",
      "\u001b[38;5;3m⚠ 3 training examples also in evaluation data\u001b[0m\n",
      "\u001b[1m\n",
      "============================== Vocab & Vectors ==============================\u001b[0m\n",
      "\u001b[38;5;4mℹ 1371274 total word(s) in the data (17507 unique)\u001b[0m\n",
      "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
      "\u001b[1m\n",
      "============================ Span Categorization ============================\u001b[0m\n",
      "\n",
      "Spans Key   Labels         \n",
      "---------   ---------------\n",
      "sc          {'U', 'R', 'M'}\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2Khuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2Khuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2Kalyzing label distribution...huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[2K\u001b[38;5;4mℹ Span characteristics for spans_key 'sc'\u001b[0m\n",
      "\u001b[38;5;4mℹ SD = Span Distinctiveness, BD = Boundary Distinctiveness\u001b[0m\n",
      "\n",
      "Span Type      Length     SD     BD       N\n",
      "------------   ------   ----   ----   -----\n",
      "R                1.48   0.21   0.11   46006\n",
      "M                1.94   0.19   0.17   16796\n",
      "U                3.00   0.23   0.28    4373\n",
      "------------   ------   ----   ----   -----\n",
      "Wgt. Average     1.69   0.20   0.13       -\n",
      "\n",
      "\u001b[38;5;4mℹ Over 90% of spans have lengths of 1 -- 4 (min=1, max=37). The most\n",
      "common span lengths are: 1 (49.27%), 2 (27.46%), 3 (12.21%), 4 (5.28%). If you\n",
      "are using the n-gram suggester, note that omitting infrequent n-gram lengths can\n",
      "greatly improve speed and memory usage.\u001b[0m\n",
      "\u001b[38;5;3m⚠ Spans may not be distinct from the rest of the corpus\u001b[0m\n",
      "\u001b[38;5;3m⚠ Boundary tokens are not distinct from the rest of the corpus\u001b[0m\n",
      "\u001b[38;5;2m✔ Good amount of examples for all labels\u001b[0m\n",
      "\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Summary ==================================\u001b[0m\n",
      "\u001b[38;5;2m✔ 4 checks passed\u001b[0m\n",
      "\u001b[38;5;3m⚠ 3 warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data /home/zlovoblachko/diploma/spacy_training/config.cfg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_realec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
