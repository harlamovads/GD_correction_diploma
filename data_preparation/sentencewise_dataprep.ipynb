{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa76516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from transformers import ElectraTokenizerFast\n",
    "from datasets import Dataset, DatasetDict\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011e7945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/zlovoblachko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "electra_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2bbe1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_correspondence = {\n",
    "    'Capitalisation': 'ORTH',\n",
    "    'Formational_affixes': 'FORM',\n",
    "    'Derivation': 'FORM',\n",
    "    'Noun_number': 'FORM',\n",
    "    'Countable_uncountable': 'FORM',\n",
    "    'Category_confusion': 'MORPH',\n",
    "    'Articles': 'DET',\n",
    "    'Determiners': 'DET',\n",
    "    'Prepositional_noun': 'POS',\n",
    "    'Prepositions': 'POS',\n",
    "    'Verb_pattern': 'POS',\n",
    "    'Conjunctions': 'POS',\n",
    "    'Pronouns': 'POS',\n",
    "    'Tense_choice': 'VERB',\n",
    "    'Tense_form': 'VERB',\n",
    "    'Voice': 'VERB',\n",
    "    'Modals': 'VERB',\n",
    "    'Numerals': 'NUM',\n",
    "    'lex_item_choice': 'WORD',\n",
    "    'Absence_comp_sent': 'WORD',\n",
    "    'Inappropriate_register': 'WORD',\n",
    "    'Ref_device': 'WORD',\n",
    "    'Linking_device': 'WORD',\n",
    "    'Punctuation': 'PUNCT',\n",
    "    'Relative_clause': 'PUNCT',\n",
    "    'Redundant_comp': 'RED',\n",
    "    'Confusion_of_structures': 'MULTIWORD',\n",
    "    'Word_order': 'MULTIWORD',\n",
    "    'Word_choice': 'MULTIWORD',\n",
    "    'Absence_explanation': 'MULTIWORD',\n",
    "    'Coherence': 'MULTIWORD',\n",
    "    'Spelling': 'SPELL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35816756",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels = [\"CORRECT\", \"ERROR\"]\n",
    "first_level_labels = [\"C\", \"M\", \"R\", \"U\"]  # C = correct, M, R, U are the first-level error types\n",
    "second_level_labels = list(set(errors_correspondence.values())) + [\"CORRECT\"]\n",
    "combined_labels = []\n",
    "for l1 in [\"M\", \"R\", \"U\"]:\n",
    "    for l2 in set(errors_correspondence.values()):\n",
    "        combined_labels.append(f\"{l1}-{l2}\")\n",
    "combined_labels.append(\"CORRECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b13fa4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassificationDataCollator:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, label_info, max_length=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_info = label_info\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        # Tokenize texts and align the labels with wordpiece tokens\n",
    "        batch = []\n",
    "        for feature in features:\n",
    "            # Get text and labels\n",
    "            text = feature[\"text\"]\n",
    "            word_labels = feature[\"word_labels\"]\n",
    "            \n",
    "            # Tokenize\n",
    "            encodings = self.tokenizer(\n",
    "                text, \n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_offsets_mapping=True\n",
    "            )\n",
    "            \n",
    "            # Align labels with tokens\n",
    "            word_ids = encodings.word_ids()\n",
    "            label_ids = []\n",
    "            \n",
    "            for word_idx in word_ids:\n",
    "                # Special tokens\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                else:\n",
    "                    # Use the label of the first token of the word\n",
    "                    try:\n",
    "                        label_ids.append(word_labels[word_idx])\n",
    "                    except IndexError:\n",
    "                        # Handle the case where word_idx is out of bounds\n",
    "                        label_ids.append(-100)\n",
    "            \n",
    "            # Add labels to encodings\n",
    "            encodings[\"labels\"] = label_ids\n",
    "            \n",
    "            # Remove offset mapping as it's not needed anymore\n",
    "            del encodings[\"offset_mapping\"]\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            batch.append({k: torch.tensor(v) for k, v in encodings.items()})\n",
    "        \n",
    "        # Concatenate all tensors\n",
    "        batch = self.tokenizer.pad(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "156c0833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_files(directory):\n",
    "    \"\"\"Read all raw files (.txt and .ann) and extract text and annotation data.\"\"\"\n",
    "    file_pairs = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt') and filename.split(\".\")[0][-1] == '1':\n",
    "            txt_path = os.path.join(directory, filename)\n",
    "            ann_path = os.path.join(directory, filename.split(\".\")[0] + \".ann\")\n",
    "            \n",
    "            if os.path.exists(ann_path):\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                with open(ann_path, 'r', encoding='utf-8') as f:\n",
    "                    annotations = f.readlines()\n",
    "                \n",
    "                file_pairs.append({\n",
    "                    'filename': filename.split(\".\")[0],\n",
    "                    'text': text,\n",
    "                    'annotations': annotations\n",
    "                })\n",
    "    \n",
    "    print(f\"Found {len(file_pairs)} text-annotation pairs\")\n",
    "    return file_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8df854c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotations(annotations):\n",
    "    \"\"\"Parse annotation lines into structured error spans with extended error type information.\"\"\"\n",
    "    error_spans = []\n",
    "    tag_to_correction = {}\n",
    "    \n",
    "    # First pass: Collect all T tags and their span information\n",
    "    for line in annotations:\n",
    "        if line.startswith('T'):\n",
    "            try:\n",
    "                parts = line.strip().split('\\t')\n",
    "                tag_id = parts[0]\n",
    "                error_info = parts[1].split(' ')\n",
    "                \n",
    "                # Skip if tag type length is 3 (these are POS tags, not error tags)\n",
    "                if len(error_info[0]) == 3:\n",
    "                    continue\n",
    "                \n",
    "                error_type = error_info[0]\n",
    "                span_start = int(error_info[1]) if error_info[1].isdigit() else None\n",
    "                span_end = int(error_info[2]) if error_info[2].isdigit() else None\n",
    "                \n",
    "                if span_start is not None and span_end is not None:\n",
    "                    error_text = parts[2].strip()\n",
    "                    \n",
    "                    # Get second-level tag based on error type\n",
    "                    second_level_tag = errors_correspondence.get(error_type, \"OTHER\")\n",
    "                    \n",
    "                    error_spans.append({\n",
    "                        'tag_id': tag_id,\n",
    "                        'error_type': error_type,\n",
    "                        'span_start': span_start,\n",
    "                        'span_end': span_end,\n",
    "                        'error_text': error_text,\n",
    "                        'correction': None,\n",
    "                        'first_level_tag': None,\n",
    "                        'second_level_tag': second_level_tag\n",
    "                    })\n",
    "            except (IndexError, ValueError) as e:\n",
    "                # Skip malformed annotation lines\n",
    "                continue\n",
    "    \n",
    "    # Second pass: Find corrections and first-level tags\n",
    "    for line in annotations:\n",
    "        if line.startswith('A'):\n",
    "            # This is a deletion annotation\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                ref_tag = None\n",
    "                for part in parts[1].split():\n",
    "                    if part.startswith('T'):\n",
    "                        ref_tag = part\n",
    "                        break\n",
    "                \n",
    "                if ref_tag:\n",
    "                    for span in error_spans:\n",
    "                        if span['tag_id'] == ref_tag:\n",
    "                            span['correction'] = \"\"\n",
    "                            span['first_level_tag'] = \"U\"  # Unnecessary (deletion)\n",
    "        \n",
    "        elif line.startswith('#') and 'lemma' not in line:\n",
    "            # This is a correction annotation\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                ref_tag = None\n",
    "                for part in parts[0].split():\n",
    "                    if part.startswith('T'):\n",
    "                        ref_tag = part\n",
    "                        break\n",
    "                \n",
    "                if ref_tag:\n",
    "                    correction = parts[2].strip()\n",
    "                    \n",
    "                    for span in error_spans:\n",
    "                        if span['tag_id'] == ref_tag:\n",
    "                            span['correction'] = correction\n",
    "                            \n",
    "                            # Determine first level tag\n",
    "                            if span['error_text'] in correction:\n",
    "                                span['first_level_tag'] = \"M\"  # Missing\n",
    "                            else:\n",
    "                                span['first_level_tag'] = \"R\"  # Replace\n",
    "    \n",
    "    return error_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5399ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using SpaCy and track character offsets.\"\"\"\n",
    "    sentences = []\n",
    "    sent_spans = []\n",
    "    \n",
    "    # Use SpaCy for sentence splitting\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sentences.append(sent.text)\n",
    "        sent_spans.append((sent.start_char, sent.end_char))\n",
    "    \n",
    "    return sentences, sent_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81b4f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_errors_to_sentences(text, error_spans, sent_spans):\n",
    "    \"\"\"Assign error spans to the appropriate sentences.\"\"\"\n",
    "    sentence_errors = [[] for _ in range(len(sent_spans))]\n",
    "    overlapping_spans = []\n",
    "    \n",
    "    # Create a map of character positions to error span indices\n",
    "    char_to_error = defaultdict(list)\n",
    "    for i, span in enumerate(error_spans):\n",
    "        for pos in range(span['span_start'], span['span_end']):\n",
    "            char_to_error[pos].append(i)\n",
    "    \n",
    "    # Find overlapping spans\n",
    "    for pos, span_indices in char_to_error.items():\n",
    "        if len(span_indices) > 1:\n",
    "            # Only add unique combinations of overlapping spans\n",
    "            overlap_set = tuple(sorted(span_indices))\n",
    "            if overlap_set not in overlapping_spans:\n",
    "                overlapping_spans.append(overlap_set)\n",
    "    \n",
    "    # Assign error spans to sentences\n",
    "    for i, (start, end) in enumerate(sent_spans):\n",
    "        for span in error_spans:\n",
    "            # Check if the error span overlaps with this sentence\n",
    "            if max(start, span['span_start']) < min(end, span['span_end']):\n",
    "                # Adjust span positions relative to the sentence start\n",
    "                adjusted_span = span.copy()\n",
    "                adjusted_span['span_start'] = max(0, span['span_start'] - start)\n",
    "                adjusted_span['span_end'] = min(end - start, span['span_end'] - start)\n",
    "                sentence_errors[i].append(adjusted_span)\n",
    "    \n",
    "    # Identify sentences with overlapping error spans\n",
    "    sentences_with_overlaps = set()\n",
    "    for overlap in overlapping_spans:\n",
    "        for span_idx in overlap:\n",
    "            span = error_spans[span_idx]\n",
    "            for i, (start, end) in enumerate(sent_spans):\n",
    "                if max(start, span['span_start']) < min(end, span['span_end']):\n",
    "                    sentences_with_overlaps.add(i)\n",
    "    \n",
    "    return sentence_errors, list(sentences_with_overlaps), overlapping_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e563bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to tokenize and align labels during training.\n",
    "    This will be called by the Trainer during training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    examples : dict\n",
    "        Dictionary of examples\n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer to use\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with tokenized inputs and aligned labels\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        # We use this argument because the texts in our dataset are lists of words and not full sentences.\n",
    "        is_split_into_words=False,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"word_labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64e50f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_dataset(file_pairs):\n",
    "    \"\"\"Create a dataset of sentences with token-level error labels.\"\"\"\n",
    "    regular_sentences = []\n",
    "    overlapping_sentences = []\n",
    "    \n",
    "    for file_data in tqdm(file_pairs, desc=\"Processing files\"):\n",
    "        text = file_data['text']\n",
    "        error_spans = parse_annotations(file_data['annotations'])\n",
    "        \n",
    "        # Split text into sentences\n",
    "        sentences, sent_spans = split_into_sentences(text)\n",
    "        \n",
    "        # Assign error spans to sentences\n",
    "        sentence_errors, sentences_with_overlaps, overlap_spans = assign_errors_to_sentences(text, error_spans, sent_spans)\n",
    "        \n",
    "        # Process each sentence\n",
    "        for i, (sentence, errors) in enumerate(zip(sentences, sentence_errors)):\n",
    "            if i in sentences_with_overlaps:\n",
    "                # Add to overlapping sentences dataset\n",
    "                overlapping_sentences.append({\n",
    "                    'text': sentence,\n",
    "                    'error_spans': errors,\n",
    "                    'file_id': file_data['filename'],\n",
    "                    'sentence_id': i\n",
    "                })\n",
    "            else:\n",
    "                # Add to regular sentences dataset\n",
    "                regular_sentences.append({\n",
    "                    'text': sentence,\n",
    "                    'error_spans': errors,\n",
    "                    'file_id': file_data['filename'],\n",
    "                    'sentence_id': i\n",
    "                })\n",
    "    \n",
    "    print(f\"Created dataset with {len(regular_sentences)} regular sentences and {len(overlapping_sentences)} sentences with overlapping errors\")\n",
    "    return regular_sentences, overlapping_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f85e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_labels(sentence_data):\n",
    "    \"\"\"Create multi-type token-level labels for each sentence.\"\"\"\n",
    "    for sentence in tqdm(sentence_data, desc=\"Creating token labels\"):\n",
    "        text = sentence['text']\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Initialize token labels (all correct by default)\n",
    "        binary_token_labels = [\"CORRECT\"] * len(doc)\n",
    "        first_level_token_labels = [\"C\"] * len(doc)  # C for correct\n",
    "        second_level_token_labels = [\"CORRECT\"] * len(doc)\n",
    "        combined_token_labels = [\"CORRECT\"] * len(doc)\n",
    "        \n",
    "        token_spans = [(token.idx, token.idx + len(token.text)) for token in doc]\n",
    "        \n",
    "        # Assign error labels to tokens\n",
    "        for error in sentence['error_spans']:\n",
    "            error_start = error['span_start']\n",
    "            error_end = error['span_end']\n",
    "            first_level = error['first_level_tag']\n",
    "            second_level = error['second_level_tag']\n",
    "            \n",
    "            if first_level is None:\n",
    "                first_level = \"R\"  # Default to R if no first level tag\n",
    "            \n",
    "            combined = f\"{first_level}-{second_level}\" if first_level != \"C\" else \"CORRECT\"\n",
    "            \n",
    "            for i, (start, end) in enumerate(token_spans):\n",
    "                # Check if token overlaps with error span\n",
    "                if max(start, error_start) < min(end, error_end):\n",
    "                    binary_token_labels[i] = \"ERROR\"\n",
    "                    first_level_token_labels[i] = first_level\n",
    "                    second_level_token_labels[i] = second_level\n",
    "                    combined_token_labels[i] = combined\n",
    "        \n",
    "        # Add token labels to the sentence data\n",
    "        sentence['tokens'] = [token.text for token in doc]\n",
    "        sentence['binary_labels'] = binary_token_labels\n",
    "        sentence['first_level_labels'] = first_level_token_labels\n",
    "        sentence['second_level_labels'] = second_level_token_labels\n",
    "        sentence['combined_labels'] = combined_token_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e0f7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_dataset(sentence_data, label_type=\"binary\"):\n",
    "    \"\"\"\n",
    "    Create a dataset in the format required by the Transformers library.\n",
    "    This version doesn't pre-tokenize the data, making it usable with any transformer model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentence_data : list\n",
    "        List of dictionaries containing sentences with token-level labels\n",
    "    label_type : str\n",
    "        Type of labels to use ('binary', 'first_level', 'second_level', or 'combined')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of examples in a format compatible with the Transformers library\n",
    "    \"\"\"\n",
    "    transformer_data = []\n",
    "    \n",
    "    # Map label names to indices based on label type\n",
    "    if label_type == \"binary\":\n",
    "        label_list = [\"CORRECT\", \"ERROR\"]\n",
    "    elif label_type == \"first_level\":\n",
    "        label_list = [\"C\", \"M\", \"R\", \"U\"]\n",
    "    elif label_type == \"second_level\":\n",
    "        label_list = list(set(errors_correspondence.values())) + [\"CORRECT\"]\n",
    "    elif label_type == \"combined\":\n",
    "        label_list = []\n",
    "        for l1 in [\"M\", \"R\", \"U\"]:\n",
    "            for l2 in set(errors_correspondence.values()):\n",
    "                label_list.append(f\"{l1}-{l2}\")\n",
    "        label_list.append(\"CORRECT\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label type: {label_type}\")\n",
    "    \n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    \n",
    "    for sentence in tqdm(sentence_data, desc=f\"Creating transformer dataset ({label_type})\"):\n",
    "        # Select the appropriate labels based on label_type\n",
    "        if label_type == \"binary\":\n",
    "            token_labels = sentence['binary_labels']\n",
    "        elif label_type == \"first_level\":\n",
    "            token_labels = sentence['first_level_labels']\n",
    "        elif label_type == \"second_level\":\n",
    "            token_labels = sentence['second_level_labels']\n",
    "        elif label_type == \"combined\":\n",
    "            token_labels = sentence['combined_labels']\n",
    "        \n",
    "        # Create a dictionary containing the text and word-level labels\n",
    "        # (not tokenized yet - will be tokenized during training)\n",
    "        example = {\n",
    "            \"text\": sentence['text'],\n",
    "            \"tokens\": sentence['tokens'],\n",
    "            \"word_labels\": [label_map.get(label, 0) for label in token_labels]\n",
    "        }\n",
    "        \n",
    "        # Add metadata\n",
    "        example[\"file_id\"] = sentence.get(\"file_id\", \"\")\n",
    "        example[\"sentence_id\"] = sentence.get(\"sentence_id\", 0)\n",
    "        \n",
    "        transformer_data.append(example)\n",
    "    \n",
    "    # Add label information to the dataset features\n",
    "    features = {\n",
    "        \"text\": {\"dtype\": \"string\"},\n",
    "        \"tokens\": {\"dtype\": \"string\", \"sequence\": True},\n",
    "        \"word_labels\": {\"dtype\": \"int32\", \"sequence\": True}\n",
    "    }\n",
    "    \n",
    "    # Include label map for reference\n",
    "    label_info = {\n",
    "        \"label_type\": label_type,\n",
    "        \"labels\": label_list,\n",
    "        \"label_map\": label_map\n",
    "    }\n",
    "    \n",
    "    return transformer_data, features, label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spacy_dataset(sentence_data, label_type=\"binary\"):\n",
    "    \"\"\"Create a dataset in the format required for SpaCy's span detection training.\"\"\"\n",
    "    spacy_data = []\n",
    "    \n",
    "    for sentence in tqdm(sentence_data, desc=f\"Creating SpaCy dataset ({label_type})\"):\n",
    "        text = sentence['text']\n",
    "        error_spans = []\n",
    "        \n",
    "        # Get appropriate token labels based on label_type\n",
    "        if label_type == \"binary\":\n",
    "            token_labels = sentence['binary_labels']\n",
    "        elif label_type == \"first_level\":\n",
    "            token_labels = sentence['first_level_labels']\n",
    "        elif label_type == \"second_level\":\n",
    "            token_labels = sentence['second_level_labels']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown label type: {label_type}\")\n",
    "        \n",
    "        # Collect spans with their labels\n",
    "        spans = []\n",
    "        for error in sentence['error_spans']:\n",
    "            error_start = error['span_start']\n",
    "            error_end = error['span_end']\n",
    "            \n",
    "            if label_type == \"binary\":\n",
    "                label = \"ERROR\"\n",
    "            elif label_type == \"first_level\":\n",
    "                label = error['first_level_tag']\n",
    "            elif label_type == \"second_level\":\n",
    "                label = error['second_level_tag']\n",
    "            \n",
    "            spans.append((error_start, error_end, label))\n",
    "        \n",
    "        # Create SpaCy training example\n",
    "        spacy_example = {\n",
    "            \"text\": text,\n",
    "            \"spans\": spans,\n",
    "            \"meta\": {\n",
    "                \"file_id\": sentence[\"file_id\"],\n",
    "                \"sentence_id\": sentence[\"sentence_id\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        spacy_data.append(spacy_example)\n",
    "    \n",
    "    return spacy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8b500e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(transformer_data, features, label_info, output_dir, hf_repo_name, dataset_type=\"binary\"):\n",
    "    \"\"\"\n",
    "    Save model-agnostic transformer dataset to disk and upload to Hugging Face.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    transformer_data : list\n",
    "        List of examples in model-agnostic format\n",
    "    features : dict\n",
    "        Dictionary describing the features of the dataset\n",
    "    label_info : dict\n",
    "        Dictionary containing label information\n",
    "    output_dir : str\n",
    "        Directory to save the dataset\n",
    "    hf_repo_name : str\n",
    "        Name of the Hugging Face repository\n",
    "    dataset_type : str\n",
    "        Type of labels in the dataset\n",
    "    \"\"\"\n",
    "    from datasets import Dataset, DatasetDict, Features, Sequence, Value\n",
    "    \n",
    "    # Convert features dictionary to HF Features format\n",
    "    hf_features = Features({\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"tokens\": Sequence(Value(\"string\")),\n",
    "        \"word_labels\": Sequence(Value(\"int32\")),\n",
    "        \"file_id\": Value(\"string\"),\n",
    "        \"sentence_id\": Value(\"int32\")\n",
    "    })\n",
    "    \n",
    "    # Create dataset from the data\n",
    "    dataset = Dataset.from_list(transformer_data, features=hf_features)\n",
    "    \n",
    "    # Train/eval/test split (80/10/10)\n",
    "    train_test_val = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    test_val = train_test_val[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "    \n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": train_test_val[\"train\"],\n",
    "        \"validation\": test_val[\"train\"],\n",
    "        \"test\": test_val[\"test\"]\n",
    "    })\n",
    "    \n",
    "    # Save dataset to disk\n",
    "    dataset_dir = os.path.join(output_dir, f\"transformer_dataset_{dataset_type}_agnostic\")\n",
    "    dataset_dict.save_to_disk(dataset_dir)\n",
    "    \n",
    "    # Save label information\n",
    "    with open(os.path.join(dataset_dir, \"label_info.json\"), \"w\") as f:\n",
    "        json.dump(label_info, f, indent=2)\n",
    "    \n",
    "    # Upload to Hugging Face\n",
    "    try:\n",
    "        repo_id = f\"{hf_repo_name}-transformer-{dataset_type}-agnostic\"\n",
    "        dataset_dict.push_to_hub(repo_id)\n",
    "        print(f\"Uploaded dataset to Hugging Face: {repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to Hugging Face: {str(e)}\")\n",
    "    \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a1dfbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9389 text-annotation pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 9389/9389 [03:56<00:00, 39.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 77004 regular sentences and 8427 sentences with overlapping errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating token labels: 100%|██████████| 77004/77004 [08:11<00:00, 156.56it/s]\n",
      "Creating token labels: 100%|██████████| 8427/8427 [00:56<00:00, 148.31it/s]\n",
      "Creating transformer dataset (binary): 100%|██████████| 77004/77004 [07:50<00:00, 163.53it/s] \n",
      "Creating transformer dataset (binary): 100%|██████████| 8427/8427 [00:52<00:00, 161.44it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6801534550b745beb9166f4ba1390614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/61603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186fda7cc100469aa4aa21fda7693b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25f2e9c22e5472fb2ec0ffc1ff3ca4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feba6e9f45e0426ea1f40bef99563961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer dataset to /home/zlovoblachko/GD_correction_diploma/data/new_datasets/transformer_dataset_binary\n",
      "Failed to upload to Hugging Face: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67f2e35a-2644a14e5dfb90e10a9770d0;b9bad6cc-daf5-4589-8a01-7a8f882ce08b)\n",
      "\n",
      "Invalid username or password.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating SpaCy dataset (binary): 100%|██████████| 77004/77004 [00:00<00:00, 690806.28it/s]\n",
      "Creating SpaCy dataset (binary): 100%|██████████| 8427/8427 [00:00<00:00, 527275.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SpaCy datasets to /home/zlovoblachko/GD_correction_diploma/data/new_datasets/spacy_dataset_binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating token labels: 100%|██████████| 77004/77004 [07:31<00:00, 170.45it/s] \n",
      "Creating token labels: 100%|██████████| 8427/8427 [00:52<00:00, 160.63it/s]\n",
      "Creating transformer dataset (first_level): 100%|██████████| 77004/77004 [07:59<00:00, 160.74it/s] \n",
      "Creating transformer dataset (first_level): 100%|██████████| 8427/8427 [00:54<00:00, 154.25it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2034bc15c0e045d08b49c5065bdf9265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/61603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7077ee0d70e94723a698dac7f3671b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a112d1b73f52446a80318d1d5bfc1183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad65bf557994c2e9ef43488fbf32a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer dataset to /home/zlovoblachko/GD_correction_diploma/data/new_datasets/transformer_dataset_first_level\n",
      "Failed to upload to Hugging Face: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/repos/create (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\"), '(Request ID: 7614224c-263c-472b-8de6-397953047d28)')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating SpaCy dataset (first_level): 100%|██████████| 77004/77004 [00:00<00:00, 713533.72it/s]\n",
      "Creating SpaCy dataset (first_level): 100%|██████████| 8427/8427 [00:00<00:00, 459222.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SpaCy datasets to /home/zlovoblachko/GD_correction_diploma/data/new_datasets/spacy_dataset_first_level\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating token labels: 100%|██████████| 77004/77004 [08:01<00:00, 160.00it/s] \n",
      "Creating token labels: 100%|██████████| 8427/8427 [00:57<00:00, 147.34it/s]\n",
      "Creating transformer dataset (second_level): 100%|██████████| 77004/77004 [07:46<00:00, 165.21it/s] \n",
      "Creating transformer dataset (second_level): 100%|██████████| 8427/8427 [00:51<00:00, 165.03it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5a284c1a5846c6a1172ee3265c95ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/61603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5247d028b90b4af7aeb5395fd3725374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961fb00b74e9492da43637bf2f5e411f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b048c655bacb465e87295dc071f12bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved transformer dataset to /home/zlovoblachko/GD_correction_diploma/data/new_datasets/transformer_dataset_second_level\n",
      "Failed to upload to Hugging Face: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67f2eb93-00fc8d797e039a6b2b9d6263;7f3cb43f-cd81-40df-b2e5-5f3a603ea82c)\n",
      "\n",
      "Invalid username or password.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating SpaCy dataset (second_level): 100%|██████████| 77004/77004 [00:00<00:00, 642532.95it/s]\n",
      "Creating SpaCy dataset (second_level): 100%|██████████| 8427/8427 [00:00<00:00, 486355.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SpaCy datasets to /home/zlovoblachko/GD_correction_diploma/data/new_datasets/spacy_dataset_second_level\n",
      "Dataset processing complete!\n"
     ]
    }
   ],
   "source": [
    "raw_dir = \"/home/zlovoblachko/GD_correction_diploma/data/rawfiles\"\n",
    "output_dir = \"/home/zlovoblachko/GD_correction_diploma/data/new_datasets\"\n",
    "hf_repo = \"Zlovoblachko/REALEC_GED\"\n",
    "process_only = [\"binary\", \"first_level\", \"second_level\"]\n",
    "file_pairs = read_raw_files(raw_dir)\n",
    "regular_sentences, overlapping_sentences = create_sentence_dataset(file_pairs)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, \"regular_sentences.json\"), \"w\") as f:\n",
    "    json.dump(regular_sentences, f, ensure_ascii=False)\n",
    "with open(os.path.join(output_dir, \"overlapping_sentences.json\"), \"w\") as f:\n",
    "    json.dump(overlapping_sentences, f, ensure_ascii=False)\n",
    "save_datasets(output_dir, hf_repo, dataset_types=process_only)\n",
    "print(\"Dataset processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37ee34df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 9389/9389 [04:06<00:00, 38.09it/s]\n",
      "Creating token labels: 100%|██████████| 77004/77004 [07:54<00:00, 162.32it/s] \n",
      "Creating token labels: 100%|██████████| 8427/8427 [00:55<00:00, 150.99it/s]\n",
      "Creating transformer dataset (binary): 100%|██████████| 77004/77004 [00:00<00:00, 464764.96it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e463f988e647968e9f303b5e02fd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/61603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e462f971cb974ee587b605e252dd0341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fe82c8192040449901d73be82a1acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183e7470fcfe4aae9a236b967b0e59a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2655853fcd04f1d81e0c6a88864b4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf35e63a2c34ea7866bff753fe2f4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd44164e77d477da3c1057b5f0aa957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafeb90a068847fa82e50545090e6e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a73ff4be7d457d84f7d3ef5f94d00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating transformer dataset (first_level): 100%|██████████| 77004/77004 [00:00<00:00, 586513.25it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c319cabfbc74b83b4d40f462f19a8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/61603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a0a68424df4f4ea3c8cbf5286ad03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052fd153bb1f4fb0ab93ff4c44a51ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaabed41cc14d77bcced29cc3fb1f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010ecb25c49f4831971cb2cc2d8ccc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108d2505ea7d468c9b4da362af1f922f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be76985205a49a48aeb135f0ac344dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2b43b3995f4b12a4efbbbbf0f90a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53036814116b465c808aa7dc0732676d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating transformer dataset (second_level): 100%|██████████| 77004/77004 [00:00<00:00, 450996.15it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e96377e0eb04bdcb9bb165a706c2247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/61603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8984662377354ce2ac71ae7bfb98aac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48485a0a167a4ca79faf8cf1b130ff19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e701fdffb1784e309c52ded54b288ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f17470a0ec49bfb7ed6f8f2b399556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40eed31e0e1445749ff6be24049dac38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5f7dcd17d340ceb23c50e3186657f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f134a0f3613478a96303cc39b6c9808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33948546b0246c79a01c1f69a4324ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating transformer dataset (combined): 100%|██████████| 77004/77004 [00:00<00:00, 116154.34it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd977052adb647959ea712cc9f4ff22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/61603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc62292ab49491caeef8a1496836040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e767ef65b740c98f75ff2d379d9386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ecb07e5f9f478b94251b2bcd857315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e661a7e37b4e16bbd8d7226e923bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e41e14990f41d7aa32f41769f09b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288aac90aff5417f9c185ac1c74d8956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb76e4ac4f54875a09d729c7d1406b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9665fe53981e43d88461e020057645d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dir = \"/home/zlovoblachko/GD_correction_diploma/data/rawfiles\"\n",
    "output_dir = \"/home/zlovoblachko/GD_correction_diploma/data/new_new_datasets\"\n",
    "hf_repo = \"Zlovoblachko/REALEC_GED\"\n",
    "process_only = [\"binary\", \"first_level\", \"second_level\", \"combined\"]\n",
    "file_pairs = read_raw_files(raw_dir)\n",
    "regular_sentences, overlapping_sentences = create_sentence_dataset(file_pairs)\n",
    "create_token_labels(regular_sentences)\n",
    "create_token_labels(overlapping_sentences)\n",
    "\n",
    "for label_type in process_only:\n",
    "    print(f\"\\nProcessing {label_type} label type...\")\n",
    "    transformer_data, features, label_info = create_transformer_dataset(\n",
    "        regular_sentences, \n",
    "        label_type=label_type\n",
    "    )\n",
    "    dataset_dict = save_datasets(\n",
    "        transformer_data, \n",
    "        features, \n",
    "        label_info, \n",
    "        output_dir, \n",
    "        hf_repo, \n",
    "        dataset_type=label_type  # Pass a single string, not a list\n",
    "    )\n",
    "    \n",
    "    print(f\"Completed processing {label_type} label type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef359045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7214142542d24fce97c088e0ef3976b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae603ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "import os\n",
    "import json\n",
    "from huggingface_hub import HfApi, login\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80adfcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_transformer_datasets(\n",
    "    base_dir,\n",
    "    hf_username,\n",
    "    repo_prefix=\"grammar-error-detection\",\n",
    "    dataset_types=[\"binary\", \"first_level\", \"second_level\", \"combined\"],\n",
    "    use_auth_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload only transformer datasets to Hugging Face.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory containing the datasets\n",
    "    hf_username : str\n",
    "        Your Hugging Face username\n",
    "    repo_prefix : str\n",
    "        Prefix for the repository names\n",
    "    dataset_types : list\n",
    "        List of dataset types to upload\n",
    "    use_auth_token : str, optional\n",
    "        Hugging Face authentication token. If None, will prompt for login.\n",
    "    \"\"\"\n",
    "    # Login to Hugging Face if token not provided\n",
    "    if use_auth_token is None:\n",
    "        login()\n",
    "    else:\n",
    "        login(token=use_auth_token)\n",
    "    \n",
    "    api = HfApi()\n",
    "    \n",
    "    # Upload transformer datasets\n",
    "    for dataset_type in dataset_types:\n",
    "        # Upload transformer dataset\n",
    "        transformer_path = os.path.join(base_dir, f\"transformer_dataset_{dataset_type}\")\n",
    "        if os.path.exists(transformer_path):\n",
    "            print(f\"Uploading transformer dataset ({dataset_type})...\")\n",
    "            \n",
    "            # Load dataset\n",
    "            dataset = load_from_disk(transformer_path)\n",
    "            \n",
    "            # Create repository name\n",
    "            repo_name = f\"{hf_username}/{repo_prefix}-transformer-{dataset_type}\"\n",
    "            \n",
    "            # Push to hub\n",
    "            try:\n",
    "                dataset.push_to_hub(repo_name)\n",
    "                print(f\"Successfully uploaded transformer dataset to {repo_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading transformer dataset ({dataset_type}): {str(e)}\")\n",
    "        else:\n",
    "            print(f\"Transformer dataset not found at {transformer_path}\")\n",
    "    \n",
    "    print(\"Transformer dataset upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0b71be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_spacy_datasets(\n",
    "    base_dir,\n",
    "    hf_username,\n",
    "    repo_prefix=\"grammar-error-detection\",\n",
    "    dataset_types=[\"binary\", \"first_level\", \"second_level\"],\n",
    "    use_auth_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Upload SpaCy datasets to Hugging Face as datasets, not models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory containing the datasets\n",
    "    hf_username : str\n",
    "        Your Hugging Face username\n",
    "    repo_prefix : str\n",
    "        Prefix for the repository names\n",
    "    dataset_types : list\n",
    "        List of dataset types to upload\n",
    "    use_auth_token : str, optional\n",
    "        Hugging Face authentication token. If None, will prompt for login.\n",
    "    \"\"\"\n",
    "    # Login to Hugging Face if token not provided\n",
    "    if use_auth_token is None:\n",
    "        login()\n",
    "    else:\n",
    "        login(token=use_auth_token)\n",
    "    \n",
    "    api = HfApi()\n",
    "    \n",
    "    # Upload SpaCy datasets\n",
    "    for dataset_type in dataset_types:\n",
    "        if dataset_type == \"combined\":\n",
    "            print(\"Skipping combined type for SpaCy dataset (not supported)\")\n",
    "            continue\n",
    "            \n",
    "        spacy_path = os.path.join(base_dir, f\"spacy_dataset_{dataset_type}\")\n",
    "        if os.path.exists(spacy_path):\n",
    "            print(f\"Uploading SpaCy dataset ({dataset_type})...\")\n",
    "            \n",
    "            # Create repository name\n",
    "            repo_name = f\"{hf_username}/{repo_prefix}-spacy-{dataset_type}\"\n",
    "            \n",
    "            # First, create the repository if it doesn't exist\n",
    "            try:\n",
    "                api.create_repo(\n",
    "                    repo_id=repo_name,\n",
    "                    repo_type=\"dataset\",  # Specify dataset type here\n",
    "                    exist_ok=True\n",
    "                )\n",
    "                print(f\"Created dataset repository: {repo_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating repository {repo_name}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Upload each JSON file directly using the API\n",
    "            for split in [\"train\", \"eval\", \"test\", \"overlapping\"]:\n",
    "                file_path = os.path.join(spacy_path, f\"{split}.json\")\n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        api.upload_file(\n",
    "                            path_or_fileobj=file_path,\n",
    "                            path_in_repo=f\"{split}.json\",\n",
    "                            repo_id=repo_name,\n",
    "                            repo_type=\"dataset\"  # Specify dataset type here\n",
    "                        )\n",
    "                        print(f\"Uploaded {split}.json to {repo_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error uploading {split}.json: {str(e)}\")\n",
    "            \n",
    "            # Create a simple README with dataset information\n",
    "            readme_content = f\"\"\"# SpaCy Grammar Error Detection Dataset - {dataset_type.capitalize()} Labels\n",
    "\n",
    "This repository contains the SpaCy-formatted dataset for grammar error detection with {dataset_type} labels.\n",
    "\n",
    "## Files:\n",
    "- `train.json`: Training data\n",
    "- `eval.json`: Evaluation data\n",
    "- `test.json`: Test data\n",
    "- `overlapping.json`: Sentences with overlapping error spans\n",
    "\n",
    "## Format:\n",
    "Each file contains a JSON array of examples in SpaCy format, with text and span annotations.\n",
    "\n",
    "## Usage:\n",
    "These files can be used directly with SpaCy's span categorization training.\n",
    "\"\"\"\n",
    "            \n",
    "            # Upload README\n",
    "            try:\n",
    "                with open(\"temp_readme.md\", \"w\") as f:\n",
    "                    f.write(readme_content)\n",
    "                \n",
    "                api.upload_file(\n",
    "                    path_or_fileobj=\"temp_readme.md\",\n",
    "                    path_in_repo=\"README.md\",\n",
    "                    repo_id=repo_name,\n",
    "                    repo_type=\"dataset\"  # Specify dataset type here\n",
    "                )\n",
    "                print(f\"Uploaded README.md to {repo_name}\")\n",
    "                \n",
    "                # Clean up temporary file\n",
    "                os.remove(\"temp_readme.md\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading README.md: {str(e)}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"SpaCy dataset not found at {spacy_path}\")\n",
    "    \n",
    "    print(\"SpaCy dataset upload complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "857ddbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ba4802381b4c3ebc83556076f68a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading transformer dataset (binary)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065b15e95490452e82d1006ce534f395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c36ca128f464feba3d412d058beba07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507ca4c4dc804cf8a8ba34226fe7f4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9d391e3f654147980e021a07cf5502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a1f2d5b0bf4a3e97f0720c0cb696a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a263a3d2a64177bc1dd802a8e57d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efdc1ee082046409097a67aa92d6a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac4ccefb5164b2c94bf74bc1a16679e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded transformer dataset to Zlovoblachko/REALEC_GED-transformer-binary\n",
      "Uploading transformer dataset (first_level)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2917699bce4b1d95271eed5a7ce182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0815474d855d40fea57f38c9111734c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e42cbc69e884b7c9bedeeabc9a9b789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84eb8b70e60a401c98338113b9408ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa3ed4943904fe0803c24db50caad6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea9133fa5f647738c55ea0139efb303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afd46ad55f64e74b086bd2b00ff402e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b161897bcb2144839c7c9c8550b8401b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded transformer dataset to Zlovoblachko/REALEC_GED-transformer-first_level\n",
      "Uploading transformer dataset (second_level)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55f843578024ecc89ceacb25d398361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e67606df65443e9e3c745984104cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f19ba649cf4f7aa3889d525b06f66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed72d44576948fcb14889854793633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7aca2b0d99740e8847382eaad1c0237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df952605554d4988ab143fb760e3d163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57cd46c228c4587b14500c4800704d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362e5ebf487e4d7d83cba05b9c731f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded transformer dataset to Zlovoblachko/REALEC_GED-transformer-second_level\n",
      "Transformer dataset upload complete!\n"
     ]
    }
   ],
   "source": [
    "upload_transformer_datasets(\n",
    "    base_dir=\"/home/zlovoblachko/GD_correction_diploma/data/new_datasets\",\n",
    "    hf_username=\"Zlovoblachko\",\n",
    "    repo_prefix=\"REALEC_GED\",\n",
    "    dataset_types=[\"binary\", \"first_level\", \"second_level\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59cf3d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef34e3855b8473ebd057c6e9a70edc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73374bf8031401a82d4b225d09d178b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/22.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zlovoblachko/miniconda3/envs/local_realec/lib/python3.12/site-packages/huggingface_hub/hf_api.py:9216: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3b74105a3748508b2e8f5d5aac1c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/22.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5019cb82f7426fb3f4ea90f8c1b583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/22.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload_spacy_datasets(\n",
    "    base_dir=\"/home/zlovoblachko/GD_correction_diploma/data/new_datasets\",\n",
    "    hf_username=\"Zlovoblachko\",\n",
    "    repo_prefix=\"REALEC_GED\",\n",
    "    dataset_types=[\"binary\", \"first_level\", \"second_level\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77eba3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def fix_and_upload_spacy_dataset(\n",
    "    repo_id,  # e.g., \"Zlovoblachko/REALEC_GED-spacy-binary\"\n",
    "    output_repo_id=None,  # Use the same repo ID by default\n",
    "    use_auth_token=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fix the SpaCy dataset format and re-upload it to be compatible with the datasets library.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    repo_id : str\n",
    "        Source repository ID\n",
    "    output_repo_id : str, optional\n",
    "        Target repository ID (defaults to source repository ID)\n",
    "    use_auth_token : str, optional\n",
    "        Hugging Face authentication token\n",
    "    \"\"\"\n",
    "    # Login to Hugging Face\n",
    "    if use_auth_token:\n",
    "        login(token=use_auth_token)\n",
    "    else:\n",
    "        login()\n",
    "    \n",
    "    if output_repo_id is None:\n",
    "        output_repo_id = repo_id\n",
    "    \n",
    "    api = HfApi()\n",
    "    \n",
    "    # Create a temporary directory for processing files\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        datasets_by_split = {}\n",
    "        \n",
    "        # Process each split\n",
    "        for split in [\"train\", \"eval\", \"test\", \"overlapping\"]:\n",
    "            try:\n",
    "                # Download the JSON file\n",
    "                file_path = api.hf_hub_download(\n",
    "                    repo_id=repo_id,\n",
    "                    filename=f\"{split}.json\",\n",
    "                    repo_type=\"dataset\"\n",
    "                )\n",
    "                \n",
    "                # Load the JSON data\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Transform the data to make it compatible with datasets\n",
    "                transformed_data = []\n",
    "                for example in data:\n",
    "                    # Convert span tuples with string labels to consistent format\n",
    "                    new_spans = []\n",
    "                    for span in example[\"spans\"]:\n",
    "                        # Make sure spans have consistent format with numeric indices and strings\n",
    "                        start, end, label = span\n",
    "                        new_spans.append({\n",
    "                            \"start\": int(start),\n",
    "                            \"end\": int(end),\n",
    "                            \"label\": str(label)\n",
    "                        })\n",
    "                    \n",
    "                    # Create a transformed example\n",
    "                    transformed_example = {\n",
    "                        \"text\": example[\"text\"],\n",
    "                        \"spans\": new_spans\n",
    "                    }\n",
    "                    \n",
    "                    # Add metadata if present\n",
    "                    if \"meta\" in example:\n",
    "                        transformed_example[\"meta\"] = example[\"meta\"]\n",
    "                    \n",
    "                    transformed_data.append(transformed_example)\n",
    "                \n",
    "                # Create and save a Hugging Face dataset\n",
    "                dataset = Dataset.from_list(transformed_data)\n",
    "                datasets_by_split[split] = dataset\n",
    "                \n",
    "                print(f\"Processed {split} split: {len(dataset)} examples\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {split} split: {str(e)}\")\n",
    "        \n",
    "        # Create a dataset dictionary if we have at least some data\n",
    "        if datasets_by_split:\n",
    "            # Create a DatasetDict\n",
    "            datasets_dict = DatasetDict(datasets_by_split)\n",
    "            \n",
    "            # Push to Hugging Face\n",
    "            try:\n",
    "                datasets_dict.push_to_hub(\n",
    "                    output_repo_id,\n",
    "                    private=False,\n",
    "                )\n",
    "                print(f\"Successfully uploaded dataset to {output_repo_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading dataset: {str(e)}\")\n",
    "        else:\n",
    "            print(\"No data to upload.\")\n",
    "\n",
    "# Example usage:\n",
    "# fix_and_upload_spacy_dataset(\"Zlovoblachko/REALEC_GED-spacy-binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7212754f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107375df2e4843a2a95bad6ca2f749b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4782f2fa844c57bd8b0f2d384350ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/22.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafed73ea44f428297bb4c5b990abfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval.json:   0%|          | 0.00/2.86M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2507c8757a7d4751978097c852dfd79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.json:   0%|          | 0.00/2.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a94094fe4d847a3af9ab933a13398f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "overlapping.json:   0%|          | 0.00/4.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baa03d725f24d71843ca822c7603f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85853c315cba48f89eff5632f94aba66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5906261e963541a3bcac25d0d7ebc93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ac8ad508a64231ada0275356a20e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739da5d02a4e4e3392d21caeaea94b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13251696bf114aa492b4bebfb36f6d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1111c221f7ef4036a714a4048f5a66db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ae1441b2c64b87a2020b25bdd9c9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc52eedb4ad44de6b79fe5a933ada13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c936fc6598644f2eab552af6334d65f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19fb197331546a8b1034e1bc958793a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/22.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18587859e6a64a2c827d22e529b787de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval.json:   0%|          | 0.00/2.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b69b18784b8411f80383bf700d98192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.json:   0%|          | 0.00/2.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e8c11059a34a8cbaf1e8434cba6638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "overlapping.json:   0%|          | 0.00/4.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64c003f783f4805b4899e48859130ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf2f86d3cff47209f61c733f017901d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb5b517c41242e898aa67c2199d4bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5400e4e0a74e5aac1edc82118466b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d89e467d1f44f1da6947fafe607ce56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249921f9ad8d42a9a9f422c787658220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1289d72a674ceda8f71d0f376f7da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91901fe6c80424c9029a1d938794faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d89785d4a154f379c3aeea2358972f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/518 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaff542bb05422bb5c67e2bb2a4ae20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2f08d568154e2892b1b7054946f7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/22.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303ad769830d47a38f15ecc580d06a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval.json:   0%|          | 0.00/2.86M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d0f3ffc70e465ab2b74d84f3049c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.json:   0%|          | 0.00/2.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cb46d18fdc40deb1ff7a49b1fd3cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "overlapping.json:   0%|          | 0.00/4.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35c5bd46ec944ab98a066bb0bc037b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f063f8000764365924b509a0dfd7d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb531a9ccb54633aecbe1d3fbc6ca0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b04fe8d08d64fccb0ee2603afe0b3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fe6f21075c4b8b90257c072cddc95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5e13a2a53f42b9b95246abf0397f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1865b0077ac3468ab4286849b3028850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eba5ceacaf64b97a90b40d09420beef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59da3ed826c4b5d96cdecba3ab3a6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/520 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Fix all SpaCy datasets\n",
    "for dataset_type in [\"binary\", \"first_level\", \"second_level\"]:\n",
    "    repo_id = f\"Zlovoblachko/REALEC_GED-spacy-{dataset_type}\"\n",
    "    fix_and_upload_spacy_dataset(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056c2979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_realec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
