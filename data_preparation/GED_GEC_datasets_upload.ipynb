{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ddc953",
   "metadata": {},
   "source": [
    "GED dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c7e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d20b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_token_dataset(input_file, output_prefix, give_splits=True, test_size=0.2, dev_size=0.1, seed=42):\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentences.append(json.loads(line))\n",
    "    token_data = []\n",
    "    for item in sentences:\n",
    "        text = item['text']\n",
    "        tokens = text.split()\n",
    "        token_labels = ['correct'] * len(tokens)\n",
    "        token_positions = []\n",
    "        current_pos = 0\n",
    "        for token in tokens:\n",
    "            start = text.find(token, current_pos)\n",
    "            end = start + len(token)\n",
    "            token_positions.append((start, end))\n",
    "            current_pos = end\n",
    "        for tag in item.get('tags', []):\n",
    "            error_start = int(tag['span_start'])\n",
    "            error_end = int(tag['span_end'])\n",
    "            for i, (token_start, token_end) in enumerate(token_positions):\n",
    "                if not (token_end <= error_start or token_start >= error_end):\n",
    "                    token_labels[i] = 'erroneous'\n",
    "        token_data.append({\n",
    "            'tokens': tokens,\n",
    "            'labels': token_labels,\n",
    "            'text': text\n",
    "        })\n",
    "    if give_splits == True:\n",
    "        train, test = train_test_split(token_data, test_size=test_size, random_state=seed)\n",
    "        train, dev = train_test_split(train, test_size=dev_size/(1-test_size), random_state=seed)\n",
    "        splits = {\n",
    "            'train': train,\n",
    "            'dev': dev,\n",
    "            'test': test\n",
    "        }\n",
    "    elif give_splits == False:\n",
    "        test = token_data\n",
    "        splits = {\n",
    "            'test' : test\n",
    "        }\n",
    "    for split_name, data in splits.items():\n",
    "        output_file = f\"{output_prefix}_{split_name}.jsonl\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        total_tokens = sum(len(item['tokens']) for item in data)\n",
    "        error_tokens = sum(label == 'erroneous' for item in data for label in item['labels'])\n",
    "        print(f\"{split_name.upper()}: {len(data)} sentences, {total_tokens} tokens\")\n",
    "        print(f\"  Errors: {error_tokens} ({error_tokens/total_tokens:.2%})\")\n",
    "        print(f\"  Saved to {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a397b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 24906 sentences, 526954 tokens\n",
      "  Errors: 62577 (11.88%)\n",
      "  Saved to bea_grammar_train.jsonl\n",
      "\n",
      "DEV: 3559 sentences, 75406 tokens\n",
      "  Errors: 8957 (11.88%)\n",
      "  Saved to bea_grammar_dev.jsonl\n",
      "\n",
      "TEST: 7117 sentences, 150908 tokens\n",
      "  Errors: 18079 (11.98%)\n",
      "  Saved to bea_grammar_test.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonl_to_token_dataset('/home/zlovoblachko/GD_correction_diploma/data_preparation/sentencewise_full.jsonl', 'bea_grammar', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5acf2b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: 5518 sentences, 119831 tokens\n",
      "  Errors: 33218 (27.72%)\n",
      "  Saved to overlapping_benchmark_test.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonl_to_token_dataset('/home/zlovoblachko/GD_correction_diploma/data_preparation/overlapping_full.jsonl', 'overlapping_benchmark', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbd278d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_four_label_token(input_file, output_prefix, give_splits=True, test_size=0.2, dev_size=0.1, seed=42):\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentences.append(json.loads(line))\n",
    "    first_level_tags = {\n",
    "        'M': 'Missing',\n",
    "        'U': 'Unnecessary',\n",
    "        'R': 'Replace',\n",
    "        'correct': 'correct'\n",
    "    }\n",
    "    token_data = []\n",
    "    for item in sentences:\n",
    "        text = item['text']\n",
    "        tokens = text.split()\n",
    "        token_labels = ['correct'] * len(tokens)\n",
    "        token_positions = []\n",
    "        current_pos = 0\n",
    "        for token in tokens:\n",
    "            start = text.find(token, current_pos)\n",
    "            end = start + len(token)\n",
    "            token_positions.append((start, end))\n",
    "            current_pos = end\n",
    "        for tag in item.get('tags', []):\n",
    "            error_start = int(tag['span_start'])\n",
    "            error_end = int(tag['span_end'])\n",
    "            first_level_tag = tag.get('first_level_tag', 'Unknown')\n",
    "            for i, (token_start, token_end) in enumerate(token_positions):\n",
    "                if not (token_end <= error_start or token_start >= error_end):\n",
    "                    token_labels[i] = first_level_tag\n",
    "        token_data.append({\n",
    "            'tokens': tokens,\n",
    "            'labels': token_labels,\n",
    "            'text': text\n",
    "        })\n",
    "    if give_splits:\n",
    "        train, test = train_test_split(token_data, test_size=test_size, random_state=seed)\n",
    "        train, dev = train_test_split(train, test_size=dev_size/(1-test_size), random_state=seed)\n",
    "        splits = {\n",
    "            'train': train,\n",
    "            'dev': dev,\n",
    "            'test': test\n",
    "        }\n",
    "    else:\n",
    "        test = token_data\n",
    "        splits = {\n",
    "            'test': test\n",
    "        }\n",
    "    for split_name, data in splits.items():\n",
    "        output_file = f\"{output_prefix}_{split_name}.jsonl\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        total_tokens = sum(len(item['tokens']) for item in data)\n",
    "        error_counts = {}\n",
    "        for item in data:\n",
    "            for label in item['labels']:\n",
    "                if label != 'correct':\n",
    "                    error_counts[label] = error_counts.get(label, 0) + 1\n",
    "        total_errors = sum(error_counts.values())\n",
    "        print(f\"{split_name.upper()}: {len(data)} sentences, {total_tokens} tokens\")\n",
    "        print(f\"  Total Errors: {total_errors} ({total_errors/total_tokens:.2%})\")\n",
    "        if error_counts:\n",
    "            print(\"  Error distribution:\")\n",
    "            for error_type, count in error_counts.items():\n",
    "                tag_name = first_level_tags.get(error_type, error_type)\n",
    "                print(f\"    {error_type} ({tag_name}): {count} ({count/total_errors:.2%})\")\n",
    "        \n",
    "        print(f\"  Saved to {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a85984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 24906 sentences, 526954 tokens\n",
      "  Total Errors: 62577 (11.88%)\n",
      "  Error distribution:\n",
      "    M (Missing): 19983 (31.93%)\n",
      "    R (Replace): 40316 (64.43%)\n",
      "    U (Unnecessary): 2278 (3.64%)\n",
      "  Saved to 4tag_grammar_GED_train.jsonl\n",
      "\n",
      "DEV: 3559 sentences, 75406 tokens\n",
      "  Total Errors: 8957 (11.88%)\n",
      "  Error distribution:\n",
      "    R (Replace): 5682 (63.44%)\n",
      "    M (Missing): 2942 (32.85%)\n",
      "    U (Unnecessary): 333 (3.72%)\n",
      "  Saved to 4tag_grammar_GED_dev.jsonl\n",
      "\n",
      "TEST: 7117 sentences, 150908 tokens\n",
      "  Total Errors: 18079 (11.98%)\n",
      "  Error distribution:\n",
      "    M (Missing): 5734 (31.72%)\n",
      "    R (Replace): 11661 (64.50%)\n",
      "    U (Unnecessary): 684 (3.78%)\n",
      "  Saved to 4tag_grammar_GED_test.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonl_to_four_label_token('/home/zlovoblachko/GD_correction_diploma/data_preparation/sentencewise_full.jsonl', '4tag_grammar_GED', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "015b188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: 5518 sentences, 119831 tokens\n",
      "  Total Errors: 33218 (27.72%)\n",
      "  Error distribution:\n",
      "    M (Missing): 6430 (19.36%)\n",
      "    R (Replace): 25426 (76.54%)\n",
      "    U (Unnecessary): 1362 (4.10%)\n",
      "  Saved to overlapping_benchmark_test.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonl_to_four_label_token('/home/zlovoblachko/GD_correction_diploma/data_preparation/overlapping_full.jsonl', 'overlapping_benchmark', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4284e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_eleven_label_token(input_file, output_prefix, give_splits=True, test_size=0.2, dev_size=0.1, seed=42):\n",
    "    sentences = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentences.append(json.loads(line))\n",
    "    \n",
    "    second_level_tags = {\n",
    "        'ORTH': 'Orthography',\n",
    "        'FORM': 'Formation',\n",
    "        'MORPH': 'Morphology',\n",
    "        'DET': 'Determiners',\n",
    "        'POS': 'Part of Speech',\n",
    "        'VERB': 'Verb Issues',\n",
    "        'NUM': 'Number Issues',\n",
    "        'WORD': 'Word Choice',\n",
    "        'PUNCT': 'Punctuation',\n",
    "        'RED': 'Redundancy',\n",
    "        'MULTIWORD': 'Multi-word Structures',\n",
    "        'SPELL': 'Spelling',\n",
    "        'correct': 'correct'\n",
    "    }\n",
    "    \n",
    "    token_data = []\n",
    "    for item in sentences:\n",
    "        text = item['text']\n",
    "        tokens = text.split()\n",
    "        token_labels = ['correct'] * len(tokens)\n",
    "        token_positions = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        # Find all token positions in the text\n",
    "        for token in tokens:\n",
    "            start = text.find(token, current_pos)\n",
    "            end = start + len(token)\n",
    "            token_positions.append((start, end))\n",
    "            current_pos = end\n",
    "        \n",
    "        # Process each error tag\n",
    "        for tag in item.get('tags', []):\n",
    "            error_start = int(tag['span_start'])\n",
    "            error_end = int(tag['span_end'])\n",
    "            \n",
    "            # Get second-level tag directly if provided\n",
    "            if 'second_level_tag' in tag:\n",
    "                second_level = tag['second_level_tag']\n",
    "            else:\n",
    "                second_level = 'UNKNOWN'\n",
    "            \n",
    "            # Find tokens that overlap with the error span\n",
    "            for i, (token_start, token_end) in enumerate(token_positions):\n",
    "                if not (token_end <= error_start or token_start >= error_end):\n",
    "                    token_labels[i] = second_level\n",
    "        \n",
    "        token_data.append({\n",
    "            'tokens': tokens,\n",
    "            'labels': token_labels,\n",
    "            'text': text\n",
    "        })\n",
    "    \n",
    "    # Create train/dev/test splits if requested\n",
    "    if give_splits:\n",
    "        train, test = train_test_split(token_data, test_size=test_size, random_state=seed)\n",
    "        train, dev = train_test_split(train, test_size=dev_size/(1-test_size), random_state=seed)\n",
    "        splits = {\n",
    "            'train': train,\n",
    "            'dev': dev,\n",
    "            'test': test\n",
    "        }\n",
    "    else:\n",
    "        test = token_data\n",
    "        splits = {\n",
    "            'test': test\n",
    "        }\n",
    "    \n",
    "    # Save and print statistics for each split\n",
    "    for split_name, data in splits.items():\n",
    "        output_file = f\"{output_prefix}_{split_name}.jsonl\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        \n",
    "        # Count tokens and errors by type\n",
    "        total_tokens = sum(len(item['tokens']) for item in data)\n",
    "        \n",
    "        # Count occurrences of each error type\n",
    "        error_counts = {}\n",
    "        for item in data:\n",
    "            for label in item['labels']:\n",
    "                if label != 'correct':\n",
    "                    error_counts[label] = error_counts.get(label, 0) + 1\n",
    "        \n",
    "        total_errors = sum(error_counts.values()) if error_counts else 0\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"{split_name.upper()}: {len(data)} sentences, {total_tokens} tokens\")\n",
    "        print(f\"  Total Errors: {total_errors} ({total_errors/total_tokens:.2%})\")\n",
    "        if error_counts:\n",
    "            print(\"  Error distribution:\")\n",
    "            for error_type, count in sorted(error_counts.items(), key=lambda x: -x[1]):\n",
    "                tag_name = second_level_tags.get(error_type, error_type)\n",
    "                print(f\"    {error_type} ({tag_name}): {count} ({count/total_errors:.2%})\")\n",
    "        \n",
    "        print(f\"  Saved to {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad9cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 24906 sentences, 526954 tokens\n",
      "  Total Errors: 62577 (11.88%)\n",
      "  Error distribution:\n",
      "    DET (Determiners): 20519 (32.79%)\n",
      "    SPELL (Spelling): 11961 (19.11%)\n",
      "    WORD (Word Choice): 5590 (8.93%)\n",
      "    MULTIWORD (Multi-word Structures): 4791 (7.66%)\n",
      "    PUNCT (Punctuation): 4445 (7.10%)\n",
      "    VERB (Verb Issues): 3899 (6.23%)\n",
      "    FORM (Formation): 3594 (5.74%)\n",
      "    POS (Part of Speech): 2736 (4.37%)\n",
      "    RED (Redundancy): 2051 (3.28%)\n",
      "    ORTH (Orthography): 1424 (2.28%)\n",
      "    NUM (Number Issues): 956 (1.53%)\n",
      "    MORPH (Morphology): 611 (0.98%)\n",
      "  Saved to 11tag_grammar_GED_train.jsonl\n",
      "\n",
      "DEV: 3559 sentences, 75406 tokens\n",
      "  Total Errors: 8957 (11.88%)\n",
      "  Error distribution:\n",
      "    DET (Determiners): 3034 (33.87%)\n",
      "    SPELL (Spelling): 1651 (18.43%)\n",
      "    WORD (Word Choice): 702 (7.84%)\n",
      "    PUNCT (Punctuation): 682 (7.61%)\n",
      "    MULTIWORD (Multi-word Structures): 666 (7.44%)\n",
      "    VERB (Verb Issues): 577 (6.44%)\n",
      "    FORM (Formation): 513 (5.73%)\n",
      "    POS (Part of Speech): 401 (4.48%)\n",
      "    RED (Redundancy): 341 (3.81%)\n",
      "    ORTH (Orthography): 166 (1.85%)\n",
      "    NUM (Number Issues): 140 (1.56%)\n",
      "    MORPH (Morphology): 84 (0.94%)\n",
      "  Saved to 11tag_grammar_GED_dev.jsonl\n",
      "\n",
      "TEST: 7117 sentences, 150908 tokens\n",
      "  Total Errors: 18079 (11.98%)\n",
      "  Error distribution:\n",
      "    DET (Determiners): 5774 (31.94%)\n",
      "    SPELL (Spelling): 3360 (18.59%)\n",
      "    WORD (Word Choice): 1629 (9.01%)\n",
      "    PUNCT (Punctuation): 1362 (7.53%)\n",
      "    MULTIWORD (Multi-word Structures): 1294 (7.16%)\n",
      "    VERB (Verb Issues): 1204 (6.66%)\n",
      "    FORM (Formation): 1033 (5.71%)\n",
      "    POS (Part of Speech): 878 (4.86%)\n",
      "    RED (Redundancy): 661 (3.66%)\n",
      "    ORTH (Orthography): 406 (2.25%)\n",
      "    NUM (Number Issues): 291 (1.61%)\n",
      "    MORPH (Morphology): 187 (1.03%)\n",
      "  Saved to 11tag_grammar_GED_test.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonl_to_eleven_label_token(jsonl_to_four_label_token('/home/zlovoblachko/GD_correction_diploma/data_preparation/sentencewise_full.jsonl', '4tag_grammar_GED', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "137bf7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: 5518 sentences, 119831 tokens\n",
      "  Total Errors: 33218 (27.72%)\n",
      "  Error distribution:\n",
      "    MULTIWORD (Multi-word Structures): 6224 (18.74%)\n",
      "    DET (Determiners): 5705 (17.17%)\n",
      "    PUNCT (Punctuation): 5215 (15.70%)\n",
      "    WORD (Word Choice): 4607 (13.87%)\n",
      "    SPELL (Spelling): 2528 (7.61%)\n",
      "    VERB (Verb Issues): 2361 (7.11%)\n",
      "    POS (Part of Speech): 1854 (5.58%)\n",
      "    FORM (Formation): 1588 (4.78%)\n",
      "    RED (Redundancy): 1488 (4.48%)\n",
      "    MORPH (Morphology): 676 (2.04%)\n",
      "    NUM (Number Issues): 517 (1.56%)\n",
      "    ORTH (Orthography): 455 (1.37%)\n",
      "  Saved to 11tag_overlapping_test.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonl_to_eleven_label_token('/home/zlovoblachko/GD_correction_diploma/data_preparation/overlapping_full.jsonl', '11tag_overlapping', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b78291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "from huggingface_hub import notebook_login, HfApi\n",
    "from datasets import ClassLabel, Value, Features, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdfaaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe4fbaa515e47e3b65ff9d8b3a02282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d3dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(file_path):\n",
    "    data = {\"tokens\": [], \"labels\": [], \"text\": []}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            data[\"tokens\"].append(item[\"tokens\"])\n",
    "            data[\"labels\"].append(item[\"labels\"])\n",
    "            data[\"text\"].append(item[\"text\"])\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20ebf408",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    \"train\": load_split(\"11tag_grammar_GED_train.jsonl\"),\n",
    "    \"validation\": load_split(\"11tag_grammar_GED_dev.jsonl\"), \n",
    "    \"test\": load_split(\"11tag_grammar_GED_test.jsonl\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "385ca867",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    'tokens': Sequence(Value('string')),\n",
    "    'labels': Sequence(ClassLabel(names=[\n",
    "        'correct', \n",
    "        'ORTH',\n",
    "        'FORM',\n",
    "        'MORPH',\n",
    "        'DET',\n",
    "        'POS',\n",
    "        'VERB',\n",
    "        'NUM',\n",
    "        'WORD',\n",
    "        'PUNCT',\n",
    "        'RED',\n",
    "        'MULTIWORD',\n",
    "        'SPELL'\n",
    "    ])),\n",
    "    'text': Value('string')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb452bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67074247874496bab213cb4797cba9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/24906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8de39d6e2204c73b2cb6d813cfa9680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/3559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec81f04769946afb7d70fee75be3c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/7117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict = dataset_dict.cast(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f948ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898c5e62b566472e8a7ff67edb445b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ac63abcae94025b893ee9e4bb8051b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6dd3f41456411ca061376a4608eda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ca2807100d43099b417fe6f4c921b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b28f1b043540db8d665e8695151b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c462b67a41c42dbace16f1d6bfbb8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Zlovoblachko/REALEC_GED_11tag_errors/commit/9f7172c4ff557b8c28c7882673c4074fdc1c860d', commit_message='Upload dataset', commit_description='', oid='9f7172c4ff557b8c28c7882673c4074fdc1c860d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Zlovoblachko/REALEC_GED_11tag_errors', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Zlovoblachko/REALEC_GED_11tag_errors'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_name = \"Zlovoblachko/REALEC_GED_11tag_errors\"\n",
    "dataset_dict.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b71887",
   "metadata": {},
   "source": [
    "GEC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5710222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a45a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_gec(json_lines):\n",
    "    data = []\n",
    "    for line in tqdm(json_lines):\n",
    "        example = json.loads(line) if isinstance(line, str) else line\n",
    "        source_text = example['text']\n",
    "        corrected_text = source_text\n",
    "        offset = 0\n",
    "        sorted_tags = sorted(example['tags'], key=lambda x: int(x['span_start']))\n",
    "        for tag in sorted_tags:\n",
    "            start = int(tag['span_start']) + offset\n",
    "            end = int(tag['span_end']) + offset\n",
    "            correction = tag['correction']\n",
    "            corrected_text = corrected_text[:start] + correction + corrected_text[end:]\n",
    "            offset += len(correction) - (end - start)\n",
    "        data.append({\n",
    "            'source': source_text,\n",
    "            'target': corrected_text\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae8b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huggingface_dataset(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    n = len(df)\n",
    "    indices = np.random.permutation(n)\n",
    "    train_idx = indices[:int(0.8 * n)]\n",
    "    val_idx = indices[int(0.8 * n):int(0.9 * n)]\n",
    "    test_idx = indices[int(0.9 * n):]\n",
    "    train_df = df.iloc[train_idx]\n",
    "    val_df = df.iloc[val_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': val_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39016d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_huggingface(dataset_dict, repo_name):\n",
    "    \"\"\"Upload the dataset to Hugging Face Hub.\"\"\"\n",
    "    dataset_dict.push_to_hub(repo_name)\n",
    "    print(f\"Dataset uploaded to {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b988e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35582/35582 [00:00<00:00, 128710.81it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7e726150174722b30425c23db6335a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63edcbecb349419fa8bd9fa9635aea96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/29 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c8b0900d084fb19466e0eb2e6b1e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117f7bea8ab84b43bbfaaffd59e94edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e9fba3cbaa48ad8b4d3fadec9c2ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84712dd4f79741718a238077925d0057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded to Zlovoblachko/REALEC_GEC_dataset\n"
     ]
    }
   ],
   "source": [
    "with open('/home/zlovoblachko/GD_correction_diploma/data_preparation/sentencewise_full.jsonl', 'r') as f:\n",
    "    json_lines = f.readlines()\n",
    "    processed_data = process_data_for_gec(json_lines)\n",
    "    dataset = create_huggingface_dataset(processed_data)\n",
    "    upload_to_huggingface(dataset, \"Zlovoblachko/REALEC_GEC_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f33d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_realec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
